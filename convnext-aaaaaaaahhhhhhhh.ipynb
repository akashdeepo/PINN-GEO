{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39763,"databundleVersionId":11756775,"sourceType":"competition"},{"sourceId":11568812,"sourceType":"datasetVersion","datasetId":7253205},{"sourceId":11569667,"sourceType":"datasetVersion","datasetId":7253605},{"sourceId":11569755,"sourceType":"datasetVersion","datasetId":7253661},{"sourceId":11887338,"sourceType":"datasetVersion","datasetId":7377931}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 0 ────────────────────────────────────────────────────────────────\n# Runtime flags for the notebook\n#\n# • RUN_TRAIN – set True to launch training\n# • RUN_VALID – set True to compute fold-0 validation MAE\n# • RUN_TEST  – set True to run ensemble inference on the test set\n#\n# Toggling these at the very top keeps downstream cells stateless.\n\nRUN_TRAIN = False   # bfloat16 or float32 recommended\nRUN_VALID = True\nRUN_TEST  = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1 ────────────────────────────────────────────────────────────────\n# Environment checks & dependency bootstrap\n#\n# • Verifies at least 2 CUDA-visible GPUs.\n# • Installs MONAI (lightweight, --no-deps) if missing.\n# • Emits a concise capability summary.\n\nimport subprocess, sys, importlib.util, textwrap, torch, os\n\ndef _install_monai():\n    print(\"Installing MONAI …\")\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--no-deps\", \"-q\", \"monai\"])\n\nif not torch.cuda.is_available() or torch.cuda.device_count() < 2:\n    raise RuntimeError(\"Requires ≥ 2 GPUs with CUDA enabled.\")\n\nif importlib.util.find_spec(\"monai\") is None:\n    _install_monai()\n\nimport monai  # noqa: E402  (import after potential installation)\n\nprint(textwrap.dedent(f\"\"\"\n    ╔══════════════════════════════════════════════╗\n    ║ CUDA devices      : {torch.cuda.device_count()}                       ║\n    ║ cuDNN available   : {torch.backends.cudnn.is_available()}             ║\n    ║ bfloat16 support  : {torch.cuda.is_bf16_supported()}                  ║\n    ║ MONAI version     : {monai.__version__}                               ║\n    ╚══════════════════════════════════════════════╝\n\"\"\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _cfg.py\n\n# Cell 2 ────────────────────────────────────────────────────────────────\n# Global config  ➜ _cfg.py\n#\n# This file stores all hyper-parameters and misc settings in a\n# SimpleNamespace for easy dot-access.  No heavyweight imports here.\n\n\nfrom types import SimpleNamespace\nimport torch, random, numpy as np, os\n\ndef set_global_seed(seed: int = 123):\n    \"\"\"Ensure deterministic-ish behaviour across libraries.\"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\ncfg = SimpleNamespace(\n    # ------------------------------------------------------------------\n    # Reproducibility\n    seed                     = 123,\n    device                   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n    local_rank               = 0,     # set by DDP launcher\n    world_size               = 1,     # set by DDP launcher\n    # ------------------------------------------------------------------\n    # Data\n    subsample                = None,  # e.g. 250 to debug faster\n    # ------------------------------------------------------------------\n    # Model\n    backbone                 = \"convnext_small.fb_in22k_ft_in1k\",\n    ema                      = True,\n    ema_decay                = 0.99,\n    # ------------------------------------------------------------------\n    # Optimisation\n    epochs                   = 1,\n    batch_size               = 16,\n    batch_size_val           = 16,\n    use_amp                  = True,\n    precision                = \"bfloat16\",    # \"float32\" | \"bfloat16\" | \"float16\"\n    gradient_accumulation_steps = 4,\n    clip_grad_norm           = 1.0,\n    warmup_epochs            = 2,\n    save_top_k               = 3,\n    # ------------------------------------------------------------------\n    # Early stopping\n    early_stopping           = {\"patience\": 3, \"streak\": 0},\n    # ------------------------------------------------------------------\n    # Logging\n    logging_steps            = 100,\n)\n\n# Make seeding immediately effective when _cfg is imported\nset_global_seed(cfg.seed)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _dataset.py\n\n# Cell 3 ────────────────────────────────────────────────────────────────\n# Dataset definition  ➜ _dataset.py\n#\n#  • Loads metadata CSV for fold splits.\n#  • Supports optional row-subsampling for quick experiments.\n#  • Performs light on-the-fly augmentation when mode == \"train\".\n#  • Uses numpy.memmap to keep RAM usage low.\n\n\nfrom __future__ import annotations\nimport os, glob, numpy as np, pandas as pd\nfrom tqdm import tqdm\nimport torch\nfrom typing import List, Tuple\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, cfg, mode: str = \"train\"):\n        \"\"\"\n        Args:\n            cfg  – global SimpleNamespace from _cfg.py\n            mode – \"train\", \"valid\", or \"test\"\n        \"\"\"\n        self.cfg   = cfg\n        self.mode  = mode\n        self.data, self.labels, self.records = self._load_metadata()\n\n    # ------------------------------------------------------------------\n    def _load_metadata(self) -> Tuple[List[np.ndarray], List[np.ndarray], List[str]]:\n        df = pd.read_csv(\"/kaggle/input/openfwi-preprocessed-72x72/folds.csv\")\n\n        if self.cfg.subsample is not None:\n            df = df.groupby([\"dataset\", \"fold\"]).head(self.cfg.subsample)\n\n        df = df[df[\"fold\"] != 0] if self.mode == \"train\" else df[df[\"fold\"] == 0]\n\n        data, labels, records = [], [], []\n        mmap_mode = \"r\"\n\n        for _, row in tqdm(df.iterrows(), total=len(df), disable=self.cfg.local_rank != 0):\n            row = row.to_dict()\n\n            # Resolve file-paths across both openfwi_float16 datasets\n            parts = row[\"data_fpath\"].split(\"/\")\n            patterns = [\n                os.path.join(\"/kaggle/input/open-wfi-1/openfwi_float16_1/\", row[\"data_fpath\"]),\n                os.path.join(\"/kaggle/input/open-wfi-1/openfwi_float16_1/\", parts[0], \"*\", parts[-1]),\n                os.path.join(\"/kaggle/input/open-wfi-2/openfwi_float16_2/\", row[\"data_fpath\"]),\n                os.path.join(\"/kaggle/input/open-wfi-2/openfwi_float16_2/\", parts[0], \"*\", parts[-1]),\n            ]\n            farr = sum((glob.glob(p) for p in patterns), [])\n            if len(farr) != 1:\n                raise FileNotFoundError(f\"Expected 1 match, got {len(farr)} for {row['data_fpath']}\")\n            fdata = farr[0]\n            flabel = fdata.replace('seis', 'vel').replace('data', 'model')\n\n            data.append(np.load(fdata, mmap_mode=mmap_mode))\n            labels.append(np.load(flabel, mmap_mode=mmap_mode))\n            records.append(row[\"dataset\"])\n\n        return data, labels, records\n\n    # ------------------------------------------------------------------\n    def __getitem__(self, idx: int):\n        row_idx, col_idx = divmod(idx, 500)\n\n        x = self.data[row_idx][col_idx].copy()\n        y = self.labels[row_idx][col_idx].copy()\n\n        if self.mode == \"train\":\n            if np.random.rand() < 0.5:        # temporal flip\n                x = x[::-1, :, ::-1]; y = y[..., ::-1]\n            if np.random.rand() < 0.3:        # gaussian noise\n                x += np.random.normal(0, 0.03, x.shape).astype(x.dtype)\n            if np.random.rand() < 0.3:        # intensity scale\n                x *= np.random.uniform(0.95, 1.05)\n            if np.random.rand() < 0.2:        # brightness jitter\n                x *= np.random.uniform(0.9, 1.1)\n            if np.random.rand() < 0.2:        # simple elastic warp\n                offset = np.random.randint(-1, 2)\n                if offset:\n                    x = np.roll(x, offset, axis=-1)\n\n        return x, y\n\n    def __len__(self) -> int:\n        return len(self.records) * 500\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _model.py\n\n\n# Cell 4 ────────────────────────────────────────────────────────────────\n# Model definition  ➜ _model.py\n#\n# • ConvNeXt encoder with custom asymmetric stem.\n# • UNet-style decoder (InstanceNorm + SCSE attention).\n# • EMA wrapper for test-time stability and Ensemble helper.\n# • Utilities to swap activations / norms and patch ConvNeXt blocks.\n\nfrom __future__ import annotations\nfrom copy import deepcopy\nfrom types import MethodType\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nfrom timm.models.convnext import ConvNeXtBlock\nfrom monai.networks.blocks import UpSample, SubpixelUpsample\n\n# ────────────────────────── EMA / Ensemble ────────────────────────────\nclass ModelEMA(nn.Module):\n    \"\"\"Exponential-moving-average wrapper.\"\"\"\n    def __init__(self, model: nn.Module, decay: float = 0.99, device: torch.device | None = None):\n        super().__init__()\n        self.module = deepcopy(model).eval()\n        self.decay  = decay\n        self.device = device\n        if self.device:\n            self.module.to(device=self.device)\n\n    @torch.no_grad()\n    def _update(self, model, fn):\n        for ema_w, w in zip(self.module.state_dict().values(), model.state_dict().values()):\n            ema_w.copy_(fn(ema_w, w.to(self.device) if self.device else w))\n\n    def update(self, model): self._update(model, lambda e, m: self.decay * e + (1.0 - self.decay) * m)\n    def set   (self, model): self._update(model, lambda _e, m: m)\n\nclass EnsembleModel(nn.Module):\n    \"\"\"Simple arithmetic-mean ensemble.\"\"\"\n    def __init__(self, models): super().__init__(); self.models = nn.ModuleList(models).eval()\n    def forward(self, x): return sum(m(x) for m in self.models) / len(self.models)\n\n# ─────────────────────────── Decoder blocks ───────────────────────────\nclass ConvBnAct2d(nn.Module):\n    def __init__(self, ic, oc, k, p=0, s=1, norm=nn.Identity, act=nn.GELU):\n        super().__init__()\n        self.conv = nn.Conv2d(ic, oc, k, stride=s, padding=p, bias=False)\n        self.norm = norm(oc) if norm is not nn.Identity else nn.Identity()\n        self.act  = act(inplace=True)\n    def forward(self, x): return self.act(self.norm(self.conv(x)))\n\nclass SCSEModule2d(nn.Module):\n    \"\"\"Spatial-and-channel squeeze-and-excitation.\"\"\"\n    def __init__(self, c, r=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(c, c // r, 1), nn.GELU(),\n            nn.Conv2d(c // r, c, 1), nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(nn.Conv2d(c, 1, 1), nn.Sigmoid())\n    def forward(self, x): return x * self.cSE(x) + x * self.sSE(x)\n\nclass Attention2d(nn.Module):\n    def __init__(self, name=None, **kw):\n        super().__init__()\n        self.attention = nn.Identity() if name is None else {\"scse\": SCSEModule2d}[name](**kw)\n    def forward(self, x): return self.attention(x)\n\nclass DecoderBlock2d(nn.Module):\n    def __init__(\n        self,\n        ic: int, skip_c: int, oc: int,\n        norm: nn.Module = nn.Identity,\n        attention: str | None = None,\n        intermediate: bool = False,\n        up: str = \"deconv\",\n        scale: int = 2,\n    ):\n        super().__init__()\n        # upsample\n        self.up = (SubpixelUpsample if up == \"pixelshuffle\" else UpSample)(\n            spatial_dims=2, in_channels=ic, out_channels=ic, scale_factor=scale, mode=up\n        )\n        # optional skip refinement\n        self.intermediate_conv = (\n            nn.Sequential(\n                ConvBnAct2d(skip_c or ic, skip_c or ic, 3, 1, norm),\n                ConvBnAct2d(skip_c or ic, skip_c or ic, 3, 1, norm),\n            ) if intermediate else None\n        )\n        self.att1 = Attention2d(attention, in_channels=ic + skip_c)\n        self.conv1 = ConvBnAct2d(ic + skip_c, oc, 3, 1, norm)\n        self.conv2 = ConvBnAct2d(oc, oc, 3, 1, norm)\n        self.att2 = Attention2d(attention, in_channels=oc)\n\n    def forward(self, x, skip=None):\n        x = self.up(x)\n        if self.intermediate_conv is not None:\n            skip = self.intermediate_conv(skip) if skip is not None else self.intermediate_conv(x)\n        if skip is not None:\n            x = self.att1(torch.cat([x, skip], dim=1))\n        x = self.conv2(self.conv1(x))\n        return self.att2(x)\n\nclass UnetDecoder2d(nn.Module):\n    def __init__(\n        self,\n        enc_chs: tuple[int],\n        dec_chs: tuple[int] = (256, 128, 64, 32),\n        norm: nn.Module = nn.Identity,\n        attention: str | None = None,\n        intermediate: bool = False,\n        up: str = \"deconv\",\n        scale_factors: tuple[int] = (2, 2, 2, 2),\n    ):\n        super().__init__()\n        if len(enc_chs) == 4:  # convnext_small has 4 stages\n            dec_chs = dec_chs[1:]\n        self.decoder_channels = dec_chs\n\n        skip_chs = list(enc_chs[1:]) + [0]\n        in_chs = [enc_chs[0]] + list(dec_chs[:-1])\n\n        self.blocks = nn.ModuleList([\n            DecoderBlock2d(ic, sc, dc, norm, attention, intermediate, up, sf)\n            for ic, sc, dc, sf in zip(in_chs, skip_chs, dec_chs, scale_factors)\n        ])\n\n    def forward(self, feats):\n        x = feats[0]\n        outs = [x]\n        skips = feats[1:]\n        for i, blk in enumerate(self.blocks):\n            x = blk(outs[-1], skip=skips[i] if i < len(skips) else None)\n            outs.append(x)\n        return outs\n\nclass SegmentationHead2d(nn.Module):\n    def __init__(self, ic, oc, scale=1, k=3, mode=\"nontrainable\"):\n        super().__init__()\n        self.conv = nn.Conv2d(ic, oc, k, padding=k // 2)\n        self.up   = UpSample(spatial_dims=2, in_channels=oc, out_channels=oc, scale_factor=scale, mode=mode)\n    def forward(self, x): return self.up(self.conv(x))\n\n# ──────────────────── ConvNeXt patch helper ---------------------------\ndef _convnext_block_forward(self, x):\n    shortcut = x\n    x = self.conv_dw(x)\n    x = self.norm(x)\n    if self.use_conv_mlp:\n        x = self.mlp(x)\n    else:\n        x = self.mlp(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2).contiguous()\n    if self.gamma is not None:\n        x = x * self.gamma.reshape(1, -1, 1, 1)\n    return self.drop_path(x) + self.shortcut(shortcut)\n\n# ───────────────────────────── Net class ──────────────────────────────\nclass Net(nn.Module):\n    def __init__(self, backbone: str, pretrained: bool = True):\n        super().__init__()\n        # Encoder\n        self.backbone = timm.create_model(\n            backbone,\n            in_chans=5,\n            pretrained=pretrained,\n            features_only=True,\n            drop_path_rate=0.0,\n        )\n        enc_chs = [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1]\n\n        # Decoder & head\n        self.decoder  = UnetDecoder2d(enc_chs)\n        self.seg_head = SegmentationHead2d(self.decoder.decoder_channels[-1], 1, scale=1)\n\n        # Customisations\n        self._update_stem(backbone)\n        self._swap_activations(self.backbone)\n        self._swap_norms(self.backbone)\n        self._patch_convnext_blocks(self.backbone)\n\n    # ---------------- stem tweak for non-square input ------------------\n    def _update_stem(self, backbone):\n        if not backbone.startswith(\"convnext\"):\n            return\n        stem = self.backbone.stem_0               # Conv2d(5→128, k7, s2)\n        stem.stride, stem.padding = (4, 1), (0, 2)\n        with torch.no_grad():\n            w = stem.weight\n            new = nn.Conv2d(w.shape[0], w.shape[0], kernel_size=(4, 4), stride=(4, 1), padding=(0, 1))\n            new.weight.copy_(w.repeat(1, (128 // w.shape[1]) + 1, 1, 1)[:, : new.weight.shape[1]])\n            new.bias.copy_(stem.bias)\n        self.backbone.stem_0 = nn.Sequential(\n            nn.ReflectionPad2d((1, 1, 80, 80)), stem, new\n        )\n\n    # --------------------- replacement utilities ----------------------\n    def _swap_activations(self, module):\n        for name, child in module.named_children():\n            if isinstance(child, (\n                nn.ReLU, nn.LeakyReLU, nn.Mish, nn.Sigmoid, nn.Tanh,\n                nn.Softmax, nn.Hardtanh, nn.ELU, nn.SELU, nn.PReLU,\n                nn.CELU, nn.GELU, nn.SiLU,\n            )):\n                setattr(module, name, nn.GELU())\n            else:\n                self._swap_activations(child)\n\n    def _swap_norms(self, module):\n        for name, child in module.named_children():\n            n_feats = None\n            if isinstance(child, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n                n_feats = child.num_features\n            elif isinstance(child, nn.GroupNorm):\n                n_feats = child.num_channels\n            elif isinstance(child, nn.LayerNorm):\n                n_feats = child.normalized_shape[0]\n\n            if n_feats is not None:\n                setattr(module, name, nn.InstanceNorm2d(n_feats, affine=True))\n            else:\n                self._swap_norms(child)\n\n    def _patch_convnext_blocks(self, module):\n        for child in module.children():\n            if isinstance(child, ConvNeXtBlock):\n                child.forward = MethodType(_convnext_block_forward, child)\n            else:\n                self._patch_convnext_blocks(child)\n\n    # -------------------------- forward pass --------------------------\n    def _forward_core(self, x_in):\n        feats = self.backbone(x_in)[::-1]      # deepest → shallowest\n        dec   = self.decoder(feats)\n        seg   = self.seg_head(dec[-1])[..., 1:-1, 1:-1]   # crop artefacts\n        return seg * 1500 + 3000\n\n    def proc_flip(self, x):                    # Test-time augmentation\n        return torch.flip(self._forward_core(torch.flip(x, dims=[-3, -1])), dims=[-1])\n\n    def forward(self, x):\n        pred = self._forward_core(x)\n        if self.training:\n            return pred\n        # two-view TTA: original + (time, space) flip\n        return torch.mean(torch.stack([pred, self.proc_flip(x)]), dim=0)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _model_init.py\n\n\n# Cell 5 ────────────────────────────────────────────────────────────────\n# Weight-initialisation & extra attention layers  ➜ _model_init.py\n#\n# • `initialize_model()` – Kaiming init for decoder & seg-head only.\n# • Optional ChannelAttention block you can swap into the decoder.\n\nfrom __future__ import annotations\nimport torch.nn as nn, torch\n\n# --------------------------------------------------------------------- #\n# 1) Improved decoder initialisation                                    #\n# --------------------------------------------------------------------- #\ndef initialize_model(model: nn.Module):\n    \"\"\"\n    Re-initialise decoder & seg-head layers with Kaiming-Normal weights\n    (backbone already carries pretrained weights).\n    \"\"\"\n    def _init(m):\n        if isinstance(m, nn.Conv2d):\n            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n            if m.bias is not None: nn.init.zeros_(m.bias)\n        elif isinstance(m, (nn.BatchNorm2d, nn.InstanceNorm2d, nn.GroupNorm)):\n            if m.weight is not None: nn.init.ones_(m.weight)\n            if m.bias   is not None: nn.init.zeros_(m.bias)\n\n    model.decoder.apply(_init)\n    model.seg_head.apply(_init)\n    return model\n\n# --------------------------------------------------------------------- #\n# 2) Channel-wise Squeeze-and-Excitation (optional)                     #\n# --------------------------------------------------------------------- #\nclass ChannelAttention(nn.Module):\n    \"\"\"\n    Lightweight SE block: global-pool → bottleneck FC → sigmoid gate.\n    Use it as a drop-in replacement for SCSE if desired.\n    \"\"\"\n    def __init__(self, in_channels: int, r: int = 16):\n        super().__init__()\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Conv2d(in_channels, in_channels // r, 1, bias=False),\n            nn.GELU(),\n            nn.Conv2d(in_channels // r, in_channels, 1, bias=False),\n            nn.Sigmoid(),\n        )\n\n    def forward(self, x):\n        return x * self.fc(self.avgpool(x))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _train_utils.py\n\n\n# Cell 6 ────────────────────────────────────────────────────────────────\n# Training utilities  ➜ _train_utils.py\n#\n# • Mixed-precision dtype selector.\n# • Warm-up + cosine LR scheduler.\n# • Checkpoint manager retaining top-K best models.\n\nfrom __future__ import annotations\nimport os, glob, math, torch, numpy as np\nfrom torch.optim import Optimizer\nfrom types import SimpleNamespace\n\n# --------------------------------------------------------------------- #\n# 1) Autocast dtype helper                                              #\n# --------------------------------------------------------------------- #\ndef get_autocast_dtype(precision: str):\n    if precision == \"bfloat16\" and torch.cuda.is_available() and torch.cuda.is_bf16_supported():\n        return torch.bfloat16\n    if precision == \"float16\":\n        return torch.float16\n    return torch.float32\n\n# --------------------------------------------------------------------- #\n# 2) Scheduler: linear warm-up → cosine decay                           #\n# --------------------------------------------------------------------- #\ndef get_lr_scheduler(optimizer: Optimizer, cfg: SimpleNamespace, steps_per_epoch: int):\n    warm = cfg.warmup_epochs * steps_per_epoch\n    total = cfg.epochs * steps_per_epoch\n\n    def lr_lambda(step: int):\n        if step < warm:                             # linear warm-up\n            return step / max(1, warm)\n        progress = (step - warm) / max(1, total - warm)\n        return 0.5 * (1.0 + math.cos(math.pi * progress))  # cosine\n\n    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\n# --------------------------------------------------------------------- #\n# 3) Checkpoint saver                                                   #\n# --------------------------------------------------------------------- #\ndef save_checkpoint(model, ema_model, optimizer, scheduler, epoch,\n                    val_loss: float, cfg: SimpleNamespace, is_best: bool):\n    if cfg.local_rank != 0:           # only rank-0 touches disk\n        return\n\n    os.makedirs(\"checkpoints\", exist_ok=True)\n    sd = (ema_model.module if (ema_model and is_best) else\n          model.module if hasattr(model, \"module\") else model).state_dict()\n\n    ckpt = {\n        \"epoch\":      epoch,\n        \"state_dict\": sd,\n        \"val_loss\":   val_loss,\n        \"optimizer\":  optimizer.state_dict(),\n        \"scheduler\":  scheduler.state_dict() if scheduler else None,\n    }\n    torch.save(ckpt, f\"checkpoints/latest_{cfg.seed}.pt\")\n    if is_best:\n        torch.save(ckpt, f\"checkpoints/best_{cfg.seed}.pt\")\n\n    # Keep only top-K best losses\n    if is_best and cfg.save_top_k > 1:\n        torch.save(ckpt, f\"checkpoints/ep{epoch}_loss{val_loss:.4f}.pt\")\n        ckpts = sorted(\n            glob.glob(f\"checkpoints/ep*_loss*.pt\"),\n            key=lambda p: float(os.path.basename(p).split(\"loss\")[1][:-3]),\n        )\n        for p in ckpts[cfg.save_top_k:]:\n            os.remove(p)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _train.py\n\n\n# Cell 7 ────────────────────────────────────────────────────────────────\n# Core trainer script  ➜ _train.py\n#\n# • DDP-aware training loop with AMP + grad-accum.\n# • Early stopping via patience counter.\n# • Minimal console noise on non-rank-0 workers.\n\nfrom __future__ import annotations\nimport os, time, random, numpy as np, torch, torch.distributed as dist\nfrom tqdm import tqdm\nfrom torch.utils.data import DistributedSampler, DataLoader\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom _cfg          import cfg\nfrom _dataset      import CustomDataset\nfrom _model        import Net, ModelEMA\nfrom _train_utils  import get_autocast_dtype, get_lr_scheduler, save_checkpoint\n\n# --------------------------------------------------------------------- #\n# 0) Reproducible seed per rank                                         #\n# --------------------------------------------------------------------- #\ndef _set_seed(seed: int):\n    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n\n# --------------------------------------------------------------------- #\n# 1) DDP setup / teardown                                               #\n# --------------------------------------------------------------------- #\ndef _ddp_setup(rank: int, world: int):\n    torch.cuda.set_device(rank)\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world)\n\ndef _ddp_cleanup():\n    dist.barrier(); dist.destroy_process_group()\n\n# --------------------------------------------------------------------- #\n# 2) Main training routine                                              #\n# --------------------------------------------------------------------- #\ndef main(cfg):\n    # ── Data ──────────────────────────────────────────────────────────\n    if cfg.local_rank == 0: print(\"⌛ Loading datasets …\", flush=True)\n    train_ds = CustomDataset(cfg, mode=\"train\")\n    valid_ds = CustomDataset(cfg, mode=\"valid\")\n\n    train_loader = DataLoader(\n        train_ds,\n        sampler=DistributedSampler(train_ds, num_replicas=cfg.world_size, rank=cfg.local_rank),\n        batch_size=cfg.batch_size,\n        num_workers=4,\n        pin_memory=True,\n    )\n    valid_loader = DataLoader(\n        valid_ds,\n        sampler=DistributedSampler(valid_ds, num_replicas=cfg.world_size, rank=cfg.local_rank),\n        batch_size=cfg.batch_size_val,\n        num_workers=4,\n        pin_memory=True,\n    )\n\n    # ── Model / Optim ────────────────────────────────────────────────\n    model = Net(cfg.backbone).to(cfg.local_rank)\n    ema_model = ModelEMA(model, cfg.ema_decay, device=cfg.local_rank) if cfg.ema else None\n    model = DDP(model, device_ids=[cfg.local_rank])\n\n    criterion  = torch.nn.L1Loss()\n    optimizer  = torch.optim.Adam(model.parameters(), lr=1e-3)\n    scaler     = GradScaler(enabled=cfg.use_amp)\n    scheduler  = get_lr_scheduler(optimizer, cfg, steps_per_epoch=len(train_loader)//cfg.gradient_accumulation_steps)\n\n    best_loss  = float(\"inf\"); val_loss = float(\"inf\")\n    dtype_autocast = get_autocast_dtype(cfg.precision)\n\n    if cfg.local_rank == 0:\n        print(f\"🚀 Training for {cfg.epochs} epoch(s) on {cfg.world_size} GPU(s)\")\n\n    # ── Epoch loop ───────────────────────────────────────────────────\n    for epoch in range(1, cfg.epochs + 1):\n        train_loader.sampler.set_epoch(epoch)\n        model.train(); epoch_loss = []\n\n        t0 = time.time(); optimizer.zero_grad()\n\n        for step, (x, y) in enumerate(train_loader, 1):\n            x, y = x.to(cfg.local_rank), y.to(cfg.local_rank)\n            with autocast(device_type=cfg.device.type, dtype=dtype_autocast, enabled=cfg.use_amp):\n                preds = model(x)\n                loss  = criterion(preds, y) / cfg.gradient_accumulation_steps\n\n            scaler.scale(loss).backward()\n\n            if step % cfg.gradient_accumulation_steps == 0 or step == len(train_loader):\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n                scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n                if ema_model: ema_model.update(model)\n\n            epoch_loss.append(loss.item() * cfg.gradient_accumulation_steps)\n\n            if cfg.local_rank == 0 and step % cfg.logging_steps == 0:\n                print(f\"Epoch {epoch} Step {step}/{len(train_loader)} \"\n                      f\"Train MAE {np.mean(epoch_loss[-cfg.logging_steps:]):.4f}\", flush=True)\n\n        # ── Validation ───────────────────────────────────────────────\n        val_loss = _validate(model, ema_model, valid_loader, criterion, cfg, dtype_autocast)\n        is_best  = val_loss < best_loss\n        best_loss = min(best_loss, val_loss)\n\n        save_checkpoint(model, ema_model, optimizer, scheduler, epoch, val_loss, cfg, is_best)\n        scheduler.step()\n\n        # ── Early stopping ───────────────────────────────────────────\n        es = cfg.early_stopping\n        es[\"streak\"] = 0 if is_best else es[\"streak\"] + 1\n        if es[\"streak\"] > es[\"patience\"]:\n            if cfg.local_rank == 0: print(\"⏹ Early stopping triggered.\"); break\n\n        if cfg.local_rank == 0:\n            dt = time.time() - t0\n            print(f\"✓ Epoch {epoch} finished | Val MAE {val_loss:.4f} | \"\n                  f\"Best {best_loss:.4f} | Time {dt/60:.1f} min\", flush=True)\n\n# --------------------------------------------------------------------- #\n# 3) Validation helper                                                  #\n# --------------------------------------------------------------------- #\n@torch.no_grad()\ndef _validate(model, ema_model, loader, criterion, cfg, dtype_autocast):\n    model.eval(); eval_model = ema_model.module if ema_model else model\n    losses = []\n    for x, y in loader:\n        x, y = x.to(cfg.local_rank), y.to(cfg.local_rank)\n        with autocast(device_type=cfg.device.type, dtype=dtype_autocast, enabled=cfg.use_amp):\n            out = eval_model(x)\n            losses.append(criterion(out, y).item())\n    loss = np.mean(losses)\n    # average across GPUs\n    v = torch.tensor([loss], device=cfg.local_rank); dist.all_reduce(v, op=dist.ReduceOp.SUM)\n    return v.item() / cfg.world_size\n\n# --------------------------------------------------------------------- #\n# 4) Entrypoint for torchrun                                            #\n# --------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    rank       = int(os.environ[\"RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n\n    _ddp_setup(rank, world_size)\n    _set_seed(cfg.seed + rank)\n\n    cfg.local_rank = rank; cfg.world_size = world_size\n    main(cfg)\n\n    _ddp_cleanup()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _train_patching.py\n\n\n# Cell 8 ────────────────────────────────────────────────────────────────\n# Optional enhanced loop patcher  ➜ _train_patching.py\n#\n# • Run `python _train_patching.py --use_improved` before torchrun to\n#   monkey-patch _train.main with an advanced loop (gradient-accum, etc.)\n\nimport argparse, importlib\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--use_improved\", action=\"store_true\", help=\"Patch _train.main with enhanced loop\")\nargs = parser.parse_args()\n\nif not args.use_improved:\n    print(\"Nothing to patch – run with --use_improved flag.\")\n    raise SystemExit\n\nprint(\"🔧  Applying enhanced training loop patch …\")\n\n# 1) import the existing modules\n_train           = importlib.import_module(\"_train\")\n_enhanced        = importlib.import_module(\"_enhanced_train_function\")\n_model_init      = importlib.import_module(\"_model_init\")\n_dataset         = importlib.import_module(\"_dataset\")\n_model           = importlib.import_module(\"_model\")\n_train_utils     = importlib.import_module(\"_train_utils\")\n\nfrom _cfg import cfg\nfrom torch.utils.data import DistributedSampler, DataLoader\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch\n\n# 2) define patched main\ndef patched_main(cfg):\n    # dataset\n    train_ds = _dataset.CustomDataset(cfg, \"train\")\n    valid_ds = _dataset.CustomDataset(cfg, \"valid\")\n    train_dl = DataLoader(\n        train_ds,\n        sampler=DistributedSampler(train_ds, num_replicas=cfg.world_size, rank=cfg.local_rank),\n        batch_size=cfg.batch_size,\n        num_workers=4,\n        pin_memory=True,\n    )\n    valid_dl = DataLoader(\n        valid_ds,\n        sampler=DistributedSampler(valid_ds, num_replicas=cfg.world_size, rank=cfg.local_rank),\n        batch_size=cfg.batch_size_val,\n        num_workers=4,\n        pin_memory=True,\n    )\n\n    # model / ema\n    model = _model.Net(cfg.backbone)\n    _model_init.initialize_model(model)      # custom init\n    model = model.to(cfg.local_rank)\n    ema   = _model.ModelEMA(model, cfg.ema_decay, device=cfg.local_rank) if cfg.ema else None\n    model = DDP(model, device_ids=[cfg.local_rank])\n\n    # optimiser & schedulers\n    criterion = torch.nn.L1Loss()\n    optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n    sched     = _train_utils.get_lr_scheduler(optim, cfg, len(train_dl)//cfg.gradient_accumulation_steps)\n\n    best = _enhanced.enhanced_training_loop(\n        model, train_dl, valid_dl, criterion, optim, sched, ema, cfg\n    )\n    return best\n\n# 3) Monkey-patch\n_train.main = patched_main\nprint(\"✅  Patch successful – launch training with torchrun as usual.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _enhanced_train_function.py\n\n\n# Cell 9 ────────────────────────────────────────────────────────────────\n# Enhanced training loop  ➜ _enhanced_train_function.py\n#\n# • Gradient accumulation + AMP + gradient-clipping.\n# • Early stopping and top-K checkpointing via _train_utils helpers.\n\nfrom __future__ import annotations\nimport time, numpy as np, torch\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\n\nfrom _train_utils import (\n    get_autocast_dtype,\n    save_checkpoint,\n)\n\n# --------------------------------------------------------------------- #\n# Core loop                                                             #\n# --------------------------------------------------------------------- #\ndef enhanced_training_loop(\n    model,\n    train_dl,\n    valid_dl,\n    criterion,\n    optimizer,\n    scheduler,\n    ema_model,\n    cfg,\n):\n    best_loss = float(\"inf\")\n    val_loss  = float(\"inf\")\n    dtype_cast = get_autocast_dtype(cfg.precision)\n    scaler     = GradScaler(enabled=cfg.use_amp)\n\n    steps_per_epoch = len(train_dl) // cfg.gradient_accumulation_steps\n    if scheduler: scheduler.optimizer.steps_per_epoch = steps_per_epoch\n\n    for epoch in range(1, cfg.epochs + 1):\n        t0 = time.time()\n        train_dl.sampler.set_epoch(epoch)\n        model.train(); epoch_losses = []\n\n        optimizer.zero_grad()\n\n        # ── Train ─────────────────────────────────────────────────────\n        for step, (x, y) in enumerate(train_dl, 1):\n            x, y = x.to(cfg.local_rank), y.to(cfg.local_rank)\n\n            with autocast(device_type=cfg.device.type, dtype=dtype_cast, enabled=cfg.use_amp):\n                preds = model(x)\n                loss  = criterion(preds, y) / cfg.gradient_accumulation_steps\n\n            scaler.scale(loss).backward()\n\n            if step % cfg.gradient_accumulation_steps == 0 or step == len(train_dl):\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.clip_grad_norm)\n                scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n                if ema_model: ema_model.update(model)\n\n            epoch_losses.append(loss.item() * cfg.gradient_accumulation_steps)\n\n            if cfg.local_rank == 0 and step % cfg.logging_steps == 0:\n                rolling = np.mean(epoch_losses[-cfg.logging_steps:])\n                print(f\"[Epoch {epoch}] Step {step}/{len(train_dl)}  \"\n                      f\"Train MAE {rolling:.4f}  Val MAE {val_loss:.4f}\", flush=True)\n\n        # ── Validation ───────────────────────────────────────────────\n        val_loss = evaluate_model(model, valid_dl, criterion, ema_model, cfg, dtype_cast)\n\n        # ── Scheduler / Checkpoint / ES ──────────────────────────────\n        if scheduler: scheduler.step()\n        is_best = val_loss < best_loss\n        best_loss = min(best_loss, val_loss)\n\n        save_checkpoint(model, ema_model, optimizer, scheduler,\n                        epoch, val_loss, cfg, is_best=is_best)\n\n        es = cfg.early_stopping\n        es[\"streak\"] = 0 if is_best else es[\"streak\"] + 1\n        if es[\"streak\"] > es[\"patience\"]:\n            if cfg.local_rank == 0: print(\"⏹  Early stopping triggered.\"); break\n\n        if cfg.local_rank == 0:\n            dt = (time.time() - t0) / 60\n            print(f\"✓ Epoch {epoch} done | Val {val_loss:.4f} | Best {best_loss:.4f} | {dt:.1f} min\")\n\n    return best_loss\n\n# --------------------------------------------------------------------- #\n# Validation helper                                                     #\n# --------------------------------------------------------------------- #\n@torch.no_grad()\ndef evaluate_model(model, loader, criterion, ema_model, cfg, dtype_cast):\n    model.eval()\n    eval_model = ema_model.module if ema_model else model\n    losses = []\n\n    for x, y in tqdm(loader, disable=cfg.local_rank != 0):\n        x, y = x.to(cfg.local_rank), y.to(cfg.local_rank)\n        with autocast(device_type=cfg.device.type, dtype=dtype_cast, enabled=cfg.use_amp):\n            out = eval_model(x)\n            losses.append(criterion(out, y).item())\n\n    loss = np.mean(losses)\n    v = torch.tensor([loss], device=cfg.local_rank); torch.distributed.all_reduce(v)\n    return v.item() / cfg.world_size\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile _optimized_test.py\n\n\n# Cell 10 ───────────────────────────────────────────────────────────────\n# Memory-efficient ensemble inference  ➜ _optimized_test.py\n#\n# • Loads checkpoints, builds EnsembleModel, runs TTA inference.\n# • Sub-batches to avoid OOM and flushes GPU cache periodically.\n\nfrom __future__ import annotations\nimport csv, time, glob, numpy as np, torch\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, SequentialSampler\n\nfrom _cfg   import cfg\nfrom _model import Net, EnsembleModel\nfrom _train_utils import get_autocast_dtype\n\n# --------------------------------------------------------------------- #\n# Dataset                                                               #\n# --------------------------------------------------------------------- #\nclass _TestDS(torch.utils.data.Dataset):\n    def __init__(self, files): self.files = files\n    def __len__(self): return len(self.files)\n    def __getitem__(self, i):\n        f = self.files[i]; stem = f.split(\"/\")[-1].split(\".\")[0]\n        return np.load(f, mmap_mode=\"r\"), stem\n\n# --------------------------------------------------------------------- #\n# Batched inference helper                                              #\n# --------------------------------------------------------------------- #\ndef _infer_batched(model, x, sub_bs, dtype_cast):\n    if sub_bs is None or sub_bs >= x.size(0):\n        with torch.cuda.amp.autocast(device_type=cfg.device.type, dtype=dtype_cast, enabled=cfg.use_amp):\n            return model(x)\n    outs = []\n    for i in range(0, x.size(0), sub_bs):\n        with torch.cuda.amp.autocast(device_type=cfg.device.type, dtype=dtype_cast, enabled=cfg.use_amp):\n            outs.append(model(x[i : i + sub_bs]))\n        if i % (sub_bs * 4) == 0:\n            torch.cuda.empty_cache()\n    return torch.cat(outs)\n\n# --------------------------------------------------------------------- #\n# Public entry-point                                                    #\n# --------------------------------------------------------------------- #\ndef run_optimized_inference():\n    print(\"🔍  Loading ensemble checkpoints …\")\n    models = []\n    for f in sorted(glob.glob(\"/kaggle/input/openfwi-preprocessed-72x72/models_1000x70/*.pt\")):\n        print(\"  ↳\", f)\n        m = Net(cfg.backbone, pretrained=False)\n        sd = torch.load(f, map_location=cfg.device, weights_only=True)\n        sd = {k.replace(\"_orig_mod.\", \"\"): v for k, v in sd.items()}\n        m.load_state_dict(sd); models.append(m)\n    model = EnsembleModel(models).to(cfg.device).eval()\n    print(f\"✅  {len(models)} models loaded.\")\n\n    dtype_cast = get_autocast_dtype(cfg.precision)\n    sub_bs     = 4  # adjust per GPU memory\n\n    test_files = sorted(glob.glob(\"/kaggle/input/open-wfi-test/test/*.npy\"))\n    dl = DataLoader(_TestDS(test_files), sampler=SequentialSampler(test_files),\n                    batch_size=cfg.batch_size_val, num_workers=4, pin_memory=True)\n\n    x_cols = [f\"x_{i}\" for i in range(1, 70, 2)]\n    fieldnames = [\"oid_ypos\"] + x_cols\n\n    t0 = time.time(); rows = 0\n    with open(\"submission.csv\", \"w\", newline=\"\") as fh:\n        writer = csv.DictWriter(fh, fieldnames); writer.writeheader()\n\n        with torch.inference_mode():\n            for x, stems in tqdm(dl):\n                x = x.to(cfg.device, non_blocking=True)\n                y = _infer_batched(model, x, sub_bs, dtype_cast).cpu().numpy()\n\n                for arr, stem in zip(y[:, 0], stems):\n                    for y_pos in range(70):\n                        writer.writerow({\n                            \"oid_ypos\": f\"{stem}_y_{y_pos}\",\n                            **{c: arr[y_pos, idx] for idx, c in enumerate(x_cols)}\n                        })\n                        rows += 1\n                if rows % 100_000 == 0: fh.flush()\n                torch.cuda.empty_cache()\n\n    print(f\"📝  submission.csv written ({rows:_} rows)  |  {time.time()-t0:.1f}s\")\n    return \"submission.csv\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 11 ───────────────────────────────────────────────────────────────\n# Quick fold-0 validation (runs only if RUN_VALID is True)\n\nif RUN_VALID:\n    from tqdm import tqdm\n    import numpy as np, torch\n    from torch.cuda.amp import autocast\n    from _dataset import CustomDataset\n    from _model   import EnsembleModel\n    from _train_utils import get_autocast_dtype\n\n    valid_ds = CustomDataset(cfg, mode=\"valid\")\n    valid_dl = torch.utils.data.DataLoader(\n        valid_ds,\n        sampler=torch.utils.data.SequentialSampler(valid_ds),\n        batch_size=cfg.batch_size_val,\n        num_workers=4,\n    )\n\n    criterion   = torch.nn.L1Loss()\n    dtype_cast  = get_autocast_dtype(cfg.precision)\n    val_logits, val_targets = [], []\n\n    with torch.no_grad():\n        for x, y in tqdm(valid_dl):\n            x, y = x.to(cfg.device), y.to(cfg.device)\n            with autocast(device_type=cfg.device.type, dtype=dtype_cast, enabled=cfg.use_amp):\n                out = model(x)\n            val_logits.append(out.cpu()); val_targets.append(y.cpu())\n\n    val_logits  = torch.cat(val_logits); val_targets = torch.cat(val_targets)\n    mae         = criterion(val_logits, val_targets).item()\n    print(f\"🔎  Fold-0 validation MAE: {mae:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 12 ───────────────────────────────────────────────────────────────\n# Test-set inference (runs only if RUN_TEST is True)\n\nif RUN_TEST:\n    try:\n        path = run_optimized_inference()\n        print(f\"✅  Submission saved → {path}\")\n    except Exception as e:\n        print(\"⚠️  Optimised path failed – falling back.  Error:\", e)\n        # ─ fallback: single-pass inference without sub-batching ─\n        import glob, csv, numpy as np, pandas as pd, time, torch\n        from torch.utils.data import DataLoader, SequentialSampler\n\n        test_files = sorted(glob.glob(\"/kaggle/input/open-wfi-test/test/*.npy\"))\n        dl = DataLoader(_TestDS(test_files), sampler=SequentialSampler(test_files),\n                        batch_size=cfg.batch_size_val, num_workers=4)\n        x_cols = [f\"x_{i}\" for i in range(1, 70, 2)]\n        fieldnames = [\"oid_ypos\"] + x_cols\n        t0 = time.time(); rows = 0\n        with open(\"submission.csv\", \"w\", newline=\"\") as fh:\n            writer = csv.DictWriter(fh, fieldnames); writer.writeheader()\n            with torch.inference_mode(), torch.cuda.amp.autocast(device_type=cfg.device.type,\n                                                                 dtype=get_autocast_dtype(cfg.precision),\n                                                                 enabled=cfg.use_amp):\n                for x, stems in tqdm(dl):\n                    preds = model(x.to(cfg.device)).cpu().numpy()[:, 0]\n                    for arr, stem in zip(preds, stems):\n                        for y_pos in range(70):\n                            writer.writerow({\n                                \"oid_ypos\": f\"{stem}_y_{y_pos}\",\n                                **{c: arr[y_pos, idx] for idx, c in enumerate(x_cols)}\n                            })\n                            rows += 1\n        print(f\"✓ Fallback inference done ({rows:_} rows) | {time.time()-t0:.1f}s\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 13 ───────────────────────────────────────────────────────────────\n# Visual sanity-check of a few predictions (only if RUN_TEST)\n\nif RUN_TEST:\n    import matplotlib.pyplot as plt, torch\n    n = 5  # number of slices to show\n    fig, axes = plt.subplots(1, n, figsize=(n * 2.5, 2.5))\n\n    # reuse last batch from Cell 12 or run a tiny inference\n    sample = torch.tensor(np.load(test_files[0]), dtype=torch.float32)[None].to(cfg.device)\n    with torch.no_grad():\n        pred = model(sample)[0, 0].cpu().numpy()\n\n    for i in range(n):\n        axes[i].imshow(pred[:, i * 2], cmap=\"gray\")\n        axes[i].set_title(f\"Slice {i}\")\n        axes[i].axis(\"off\")\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}