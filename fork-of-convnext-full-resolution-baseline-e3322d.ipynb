{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39763,"databundleVersionId":11756775,"sourceType":"competition"},{"sourceId":11568812,"sourceType":"datasetVersion","datasetId":7253205},{"sourceId":11569667,"sourceType":"datasetVersion","datasetId":7253605},{"sourceId":11569755,"sourceType":"datasetVersion","datasetId":7253661},{"sourceId":11887338,"sourceType":"datasetVersion","datasetId":7377931},{"sourceId":11910577,"sourceType":"datasetVersion","datasetId":7487776}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Enhanced ConvNeXt Seismic Inversion with L1+SSIM Loss\n# This notebook improves upon the baseline with better loss function and training\n\n# Configuration\nRUN_TRAIN = True\nRUN_VALID = True  \nRUN_TEST = True\n\nimport torch\nif not torch.cuda.is_available() or torch.cuda.device_count() < 2:\n    raise RuntimeError(\"Requires >= 2 GPUs with CUDA enabled.\")\n\n# Install dependencies\ntry: \n    import monai\nexcept: \n    !pip install --no-deps monai -q\n\ntry:\n    import piq\nexcept:\n    !pip install piq -q\n\nprint(\"üöÄ Enhanced ConvNeXt Training with L1+SSIM Loss\")\nprint(\"=\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:40:04.858182Z","iopub.execute_input":"2025-05-23T03:40:04.858536Z","iopub.status.idle":"2025-05-23T03:40:30.738739Z","shell.execute_reply.started":"2025-05-23T03:40:04.858514Z","shell.execute_reply":"2025-05-23T03:40:30.738070Z"}},"outputs":[{"name":"stderr","text":"<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-05-23 03:40:19.564554: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747971619.762520      79 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747971619.822010      79 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"üöÄ Enhanced ConvNeXt Training with L1+SSIM Loss\n==================================================\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile _cfg.py\n\nfrom types import SimpleNamespace\nimport torch\n\ncfg = SimpleNamespace()\ncfg.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncfg.local_rank = 0\ncfg.seed = 123\ncfg.subsample = None\n\ncfg.backbone = \"convnext_small.fb_in22k_ft_in1k\"\ncfg.ema = True\ncfg.ema_decay = 0.999\n\n# Enhanced training parameters\ncfg.epochs = 50\ncfg.batch_size = 16\ncfg.batch_size_val = 16\n\ncfg.early_stopping = {\"patience\": 7, \"streak\": 0}\ncfg.logging_steps = 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:40:30.739807Z","iopub.execute_input":"2025-05-23T03:40:30.740467Z","iopub.status.idle":"2025-05-23T03:40:30.745681Z","shell.execute_reply.started":"2025-05-23T03:40:30.740445Z","shell.execute_reply":"2025-05-23T03:40:30.744800Z"}},"outputs":[{"name":"stdout","text":"Overwriting _cfg.py\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile _dataset.py\n\nimport os\nimport glob\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(\n        self, \n        cfg,\n        mode = \"train\", \n    ):\n        self.cfg = cfg\n        self.mode = mode\n        \n        self.data, self.labels, self.records = self.load_metadata()\n\n    def load_metadata(self, ):\n\n        # Select rows\n        df= pd.read_csv(\"/kaggle/input/openfwi-preprocessed-72x72/folds.csv\")\n        if self.cfg.subsample is not None:\n            df= df.groupby([\"dataset\", \"fold\"]).head(self.cfg.subsample)\n\n        if self.mode == \"train\":\n            df= df[df[\"fold\"] != 0]\n        else:\n            df= df[df[\"fold\"] == 0]\n\n        \n        data = []\n        labels = []\n        records = []\n        mmap_mode = \"r\"\n\n        for idx, row in tqdm(df.iterrows(), total=len(df), disable=self.cfg.local_rank != 0):\n            row= row.to_dict()\n\n            # Hacky way to get exact file name\n            p1 = os.path.join(\"/kaggle/input/open-wfi-1/openfwi_float16_1/\", row[\"data_fpath\"])\n            p2 = os.path.join(\"/kaggle/input/open-wfi-1/openfwi_float16_1/\", row[\"data_fpath\"].split(\"/\")[0], \"*\", row[\"data_fpath\"].split(\"/\")[-1])\n            p3 = os.path.join(\"/kaggle/input/open-wfi-2/openfwi_float16_2/\", row[\"data_fpath\"])\n            p4 = os.path.join(\"/kaggle/input/open-wfi-2/openfwi_float16_2/\", row[\"data_fpath\"].split(\"/\")[0], \"*\", row[\"data_fpath\"].split(\"/\")[-1])\n            farr= glob.glob(p1) + glob.glob(p2) + glob.glob(p3) + glob.glob(p4)\n        \n            # Map to lbl fpath\n            farr= farr[0]\n            flbl= farr.replace('seis', 'vel').replace('data', 'model')\n            \n            # Load\n            arr= np.load(farr, mmap_mode=mmap_mode)\n            lbl= np.load(flbl, mmap_mode=mmap_mode)\n\n            # Append\n            data.append(arr)\n            labels.append(lbl)\n            records.append(row[\"dataset\"])\n\n        return data, labels, records\n\n    def __getitem__(self, idx):\n        row_idx= idx // 500\n        col_idx= idx % 500\n\n        d= self.records[row_idx]\n        x= self.data[row_idx][col_idx, ...]\n        y= self.labels[row_idx][col_idx, ...]\n\n        # Augs \n        if self.mode == \"train\":\n            \n            # Temporal flip\n            if np.random.random() < 0.5:\n                x= x[::-1, :, ::-1]\n                y= y[..., ::-1]\n\n        x= x.copy()\n        y= y.copy()\n        \n        return x, y\n\n    def __len__(self, ):\n        return len(self.records) * 500","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:40:30.746460Z","iopub.execute_input":"2025-05-23T03:40:30.746973Z","iopub.status.idle":"2025-05-23T03:40:30.760817Z","shell.execute_reply.started":"2025-05-23T03:40:30.746951Z","shell.execute_reply":"2025-05-23T03:40:30.760228Z"}},"outputs":[{"name":"stdout","text":"Overwriting _dataset.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"%%writefile _model.py\n\nfrom copy import deepcopy\nfrom types import MethodType\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport timm\nfrom timm.models.convnext import ConvNeXtBlock\n\nfrom monai.networks.blocks import UpSample, SubpixelUpsample\nfrom piq import ssim\n\n####################\n## Enhanced Loss  ##\n####################\n\nclass L1_SSIM_Loss(nn.Module):\n    def __init__(self, alpha=0.85):\n        super().__init__()\n        self.alpha = alpha\n        self.l1 = nn.L1Loss()\n\n    def forward(self, pred, target):\n        # Compute L1 in whatever dtype we're using\n        l1 = self.l1(pred, target)\n\n        # Switch to float32 for SSIM\n        pred = pred.float()\n        target = target.float()\n\n        min_val = target.amin()\n        max_val = target.amax()\n        eps = 1e-6\n\n        pred_norm = (pred - min_val) / (max_val - min_val + eps)\n        target_norm = (target - min_val) / (max_val - min_val + eps)\n\n        pred_norm = pred_norm.clamp(0.0, 1.0)\n        target_norm = target_norm.clamp(0.0, 1.0)\n\n        ssim_val = ssim(pred_norm, target_norm, data_range=1.0)\n\n        return self.alpha * l1 + (1.0 - self.alpha) * (1.0 - ssim_val)\n\n####################\n## EMA + Ensemble ##\n####################\n\nclass ModelEMA(nn.Module):\n    def __init__(self, model, decay=0.99, device=None):\n        super().__init__()\n        self.module = deepcopy(model)\n        self.module.eval()\n        self.decay = decay\n        self.device = device\n        if self.device is not None:\n            self.module.to(device=device)\n\n    def _update(self, model, update_fn):\n        with torch.no_grad():\n            for ema_v, model_v in zip(self.module.state_dict().values(), model.state_dict().values()):\n                if self.device is not None:\n                    model_v = model_v.to(device=self.device)\n                ema_v.copy_(update_fn(ema_v, model_v))\n\n    def update(self, model):\n        self._update(model, update_fn=lambda e, m: self.decay * e + (1. - self.decay) * m)\n\n    def set(self, model):\n        self._update(model, update_fn=lambda e, m: m)\n\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, models):\n        super().__init__()\n        self.models = nn.ModuleList(models).eval()\n\n    def forward(self, x):\n        output = None\n        \n        for m in self.models:\n            logits= m(x)\n            \n            if output is None:\n                output = logits\n            else:\n                output += logits\n                \n        output /= len(self.models)\n        return output\n\n#############\n## Decoder ##\n#############\n\nclass ConvBnAct2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        padding: int = 0,\n        stride: int = 1,\n        norm_layer: nn.Module = nn.Identity,\n        act_layer: nn.Module = nn.ReLU,\n    ):\n        super().__init__()\n\n        self.conv= nn.Conv2d(\n            in_channels, \n            out_channels,\n            kernel_size,\n            stride=stride, \n            padding=padding, \n            bias=False,\n        )\n        self.norm = norm_layer(out_channels) if norm_layer != nn.Identity else nn.Identity()\n        self.act= act_layer(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.norm(x)\n        x = self.act(x)\n        return x\n\n\nclass SCSEModule2d(nn.Module):\n    def __init__(self, in_channels, reduction=16):\n        super().__init__()\n        self.cSE = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, in_channels // reduction, 1),\n            nn.Tanh(),\n            nn.Conv2d(in_channels // reduction, in_channels, 1),\n            nn.Sigmoid(),\n        )\n        self.sSE = nn.Sequential(\n            nn.Conv2d(in_channels, 1, 1), \n            nn.Sigmoid(),\n            )\n\n    def forward(self, x):\n        return x * self.cSE(x) + x * self.sSE(x)\n\nclass Attention2d(nn.Module):\n    def __init__(self, name, **params):\n        super().__init__()\n        if name is None:\n            self.attention = nn.Identity(**params)\n        elif name == \"scse\":\n            self.attention = SCSEModule2d(**params)\n        else:\n            raise ValueError(\"Attention {} is not implemented\".format(name))\n\n    def forward(self, x):\n        return self.attention(x)\n\nclass DecoderBlock2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        skip_channels,\n        out_channels,\n        norm_layer: nn.Module = nn.Identity,\n        attention_type: str = None,\n        intermediate_conv: bool = False,\n        upsample_mode: str = \"deconv\",\n        scale_factor: int = 2,\n    ):\n        super().__init__()\n\n        # Upsample block\n        if upsample_mode == \"pixelshuffle\":\n            self.upsample= SubpixelUpsample(\n                spatial_dims= 2,\n                in_channels= in_channels,\n                scale_factor= scale_factor,\n            )\n        else:\n            self.upsample = UpSample(\n                spatial_dims= 2,\n                in_channels= in_channels,\n                out_channels= in_channels,\n                scale_factor= scale_factor,\n                mode= upsample_mode,\n            )\n\n        if intermediate_conv:\n            k= 3\n            c= skip_channels if skip_channels != 0 else in_channels\n            self.intermediate_conv = nn.Sequential(\n                ConvBnAct2d(c, c, k, k//2),\n                ConvBnAct2d(c, c, k, k//2),\n                )\n        else:\n            self.intermediate_conv= None\n\n        self.attention1 = Attention2d(\n            name= attention_type, \n            in_channels= in_channels + skip_channels,\n            )\n\n        self.conv1 = ConvBnAct2d(\n            in_channels + skip_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n            norm_layer= norm_layer,\n        )\n\n        self.conv2 = ConvBnAct2d(\n            out_channels,\n            out_channels,\n            kernel_size= 3,\n            padding= 1,\n            norm_layer= norm_layer,\n        )\n        self.attention2 = Attention2d(\n            name= attention_type, \n            in_channels= out_channels,\n            )\n\n    def forward(self, x, skip=None):\n        x = self.upsample(x)\n\n        if self.intermediate_conv is not None:\n            if skip is not None:\n                skip = self.intermediate_conv(skip)\n            else:\n                x = self.intermediate_conv(x)\n\n        if skip is not None:\n            x = torch.cat([x, skip], dim=1)\n            x = self.attention1(x)\n\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.attention2(x)\n        return x\n\n\nclass UnetDecoder2d(nn.Module):\n    def __init__(\n        self,\n        encoder_channels: tuple[int],\n        skip_channels: tuple[int] = None,\n        decoder_channels: tuple = (256, 128, 64, 32),\n        scale_factors: tuple = (2,2,2,2),\n        norm_layer: nn.Module = nn.Identity,\n        attention_type: str = None,\n        intermediate_conv: bool = False,\n        upsample_mode: str = \"deconv\",\n    ):\n        super().__init__()\n        \n        if len(encoder_channels) == 4:\n            decoder_channels= decoder_channels[1:]\n        self.decoder_channels= decoder_channels\n        \n        if skip_channels is None:\n            skip_channels= list(encoder_channels[1:]) + [0]\n\n        # Build decoder blocks\n        in_channels= [encoder_channels[0]] + list(decoder_channels[:-1])\n        self.blocks = nn.ModuleList()\n\n        for i, (ic, sc, dc) in enumerate(zip(in_channels, skip_channels, decoder_channels)):\n            self.blocks.append(\n                DecoderBlock2d(\n                    ic, sc, dc, \n                    norm_layer= norm_layer,\n                    attention_type= attention_type,\n                    intermediate_conv= intermediate_conv,\n                    upsample_mode= upsample_mode,\n                    scale_factor= scale_factors[i],\n                    )\n            )\n\n    def forward(self, feats: list[torch.Tensor]):\n        res= [feats[0]]\n        feats= feats[1:]\n\n        # Decoder blocks\n        for i, b in enumerate(self.blocks):\n            skip= feats[i] if i < len(feats) else None\n            res.append(\n                b(res[-1], skip=skip),\n                )\n            \n        return res\n\nclass SegmentationHead2d(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        scale_factor: tuple[int] = (2,2),\n        kernel_size: int = 3,\n        mode: str = \"nontrainable\",\n    ):\n        super().__init__()\n        self.conv= nn.Conv2d(\n            in_channels, out_channels, kernel_size= kernel_size,\n            padding= kernel_size//2\n        )\n        self.upsample = UpSample(\n            spatial_dims= 2,\n            in_channels= out_channels,\n            out_channels= out_channels,\n            scale_factor= scale_factor,\n            mode= mode,\n        )\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.upsample(x)\n        return x\n\n#############\n## Encoder ##\n#############\n\ndef _convnext_block_forward(self, x):\n    shortcut = x\n    x = self.conv_dw(x)\n\n    if self.use_conv_mlp:\n        x = self.norm(x)\n        x = self.mlp(x)\n    else:\n        x = self.norm(x)\n        x = x.permute(0, 2, 3, 1)\n        x = x.contiguous()\n        x = self.mlp(x)\n        x = x.permute(0, 3, 1, 2)\n        x = x.contiguous()\n\n    if self.gamma is not None:\n        x = x * self.gamma.reshape(1, -1, 1, 1)\n\n    x = self.drop_path(x) + self.shortcut(shortcut)\n    return x\n\n\nclass Net(nn.Module):\n    def __init__(\n        self,\n        backbone: str,\n        pretrained: bool = True,\n    ):\n        super().__init__()\n        \n        # Encoder\n        self.backbone= timm.create_model(\n            backbone,\n            in_chans= 5,\n            pretrained= pretrained,\n            features_only= True,\n            drop_path_rate=0.0,\n            )\n        ecs= [_[\"num_chs\"] for _ in self.backbone.feature_info][::-1]\n\n        # Decoder\n        self.decoder= UnetDecoder2d(\n            encoder_channels= ecs,\n        )\n\n        self.seg_head= SegmentationHead2d(\n            in_channels= self.decoder.decoder_channels[-1],\n            out_channels= 1,\n            scale_factor= 1,\n        )\n        \n        self._update_stem(backbone)\n        \n        self.replace_activations(self.backbone, log=True)\n        self.replace_norms(self.backbone, log=True)\n        self.replace_forwards(self.backbone, log=True)\n\n    def _update_stem(self, backbone):\n        if backbone.startswith(\"convnext\"):\n\n            # Update stride\n            self.backbone.stem_0.stride = (4, 1)\n            self.backbone.stem_0.padding = (0, 2)\n\n            # Duplicate stem layer (to downsample height)\n            with torch.no_grad():\n                w = self.backbone.stem_0.weight\n                new_conv= nn.Conv2d(w.shape[0], w.shape[0], kernel_size=(4, 4), stride=(4, 1), padding=(0, 1))\n                new_conv.weight.copy_(w.repeat(1, (128//w.shape[1])+1, 1, 1)[:, :new_conv.weight.shape[1], :, :])\n                new_conv.bias.copy_(self.backbone.stem_0.bias)\n\n            self.backbone.stem_0= nn.Sequential(\n                nn.ReflectionPad2d((1,1,80,80)),\n                self.backbone.stem_0,\n                new_conv,\n            )\n\n        else:\n            raise ValueError(\"Custom striding not implemented.\")\n        pass\n\n    def replace_activations(self, module, log=False):\n        if log:\n            print(f\"Replacing all activations with GELU...\")\n        \n        # Apply activations\n        for name, child in module.named_children():\n            if isinstance(child, (\n                nn.ReLU, nn.LeakyReLU, nn.Mish, nn.Sigmoid, \n                nn.Tanh, nn.Softmax, nn.Hardtanh, nn.ELU, \n                nn.SELU, nn.PReLU, nn.CELU, nn.GELU, nn.SiLU,\n            )):\n                setattr(module, name, nn.GELU())\n            else:\n                self.replace_activations(child)\n\n    def replace_norms(self, mod, log=False):\n        if log:\n            print(f\"Replacing all norms with InstanceNorm...\")\n            \n        for name, c in mod.named_children():\n\n            # Get feature size\n            n_feats= None\n            if isinstance(c, (nn.BatchNorm2d, nn.InstanceNorm2d)):\n                n_feats= c.num_features\n            elif isinstance(c, (nn.GroupNorm,)):\n                n_feats= c.num_channels\n            elif isinstance(c, (nn.LayerNorm,)):\n                n_feats= c.normalized_shape[0]\n\n            if n_feats is not None:\n                new = nn.InstanceNorm2d(\n                    n_feats,\n                    affine=True,\n                    )\n                setattr(mod, name, new)\n            else:\n                self.replace_norms(c)\n\n    def replace_forwards(self, mod, log=False):\n        if log:\n            print(f\"Replacing forward functions...\")\n            \n        for name, c in mod.named_children():\n            if isinstance(c, ConvNeXtBlock):\n                c.forward = MethodType(_convnext_block_forward, c)\n            else:\n                self.replace_forwards(c)\n\n        \n    def proc_flip(self, x_in):\n        x_in= torch.flip(x_in, dims=[-3, -1])\n        x= self.backbone(x_in)\n        x= x[::-1]\n\n        # Decoder\n        x= self.decoder(x)\n        x_seg= self.seg_head(x[-1])\n        x_seg= x_seg[..., 1:-1, 1:-1]\n        x_seg= torch.flip(x_seg, dims=[-1])\n        x_seg= x_seg * 1500 + 3000\n        return x_seg\n\n    def forward(self, batch):\n        x= batch\n\n        # Encoder\n        x_in = x\n        x= self.backbone(x)\n        x= x[::-1]\n\n        # Decoder\n        x= self.decoder(x)\n        x_seg= self.seg_head(x[-1])\n        x_seg= x_seg[..., 1:-1, 1:-1]\n        x_seg= x_seg * 1500 + 3000\n    \n        if self.training:\n            return x_seg\n        else:\n            p1 = self.proc_flip(x_in)\n            x_seg = torch.mean(torch.stack([x_seg, p1]), dim=0)\n            return x_seg","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:40:30.762144Z","iopub.execute_input":"2025-05-23T03:40:30.762340Z","iopub.status.idle":"2025-05-23T03:40:30.778342Z","shell.execute_reply.started":"2025-05-23T03:40:30.762325Z","shell.execute_reply":"2025-05-23T03:40:30.777850Z"}},"outputs":[{"name":"stdout","text":"Overwriting _model.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"%%writefile _utils.py\n\nimport datetime\n\ndef format_time(elapsed):\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:40:30.778997Z","iopub.execute_input":"2025-05-23T03:40:30.779166Z","iopub.status.idle":"2025-05-23T03:40:30.792452Z","shell.execute_reply.started":"2025-05-23T03:40:30.779153Z","shell.execute_reply":"2025-05-23T03:40:30.791914Z"}},"outputs":[{"name":"stdout","text":"Overwriting _utils.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"%%writefile _train.py\n\nimport os\nimport time \nimport random\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.amp import autocast, GradScaler\n\nimport torch.distributed as dist\nfrom torch.utils.data import DistributedSampler\nfrom torch.nn.parallel import DistributedDataParallel\n\nfrom _cfg import cfg\nfrom _dataset import CustomDataset\nfrom _model import ModelEMA, Net, L1_SSIM_Loss\nfrom _utils import format_time\n\ndef set_seed(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\ndef setup(rank, world_size):\n    torch.cuda.set_device(rank)\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    return\n\ndef cleanup():\n    dist.barrier()\n    dist.destroy_process_group()\n    return\n\ndef main(cfg):\n\n    # ========== Datasets / Dataloaders ==========\n    if cfg.local_rank == 0:\n        print(\"=\"*50)\n        print(\"üî• ENHANCED TRAINING WITH L1+SSIM LOSS\")\n        print(\"=\"*50)\n        print(\"Loading data..\")\n    train_ds = CustomDataset(cfg=cfg, mode=\"train\")\n    sampler= DistributedSampler(\n        train_ds, \n        num_replicas=cfg.world_size, \n        rank=cfg.local_rank,\n    )\n    train_dl = torch.utils.data.DataLoader(\n        train_ds, \n        sampler= sampler,\n        batch_size= cfg.batch_size, \n        num_workers= 4,\n    )\n    \n    valid_ds = CustomDataset(cfg=cfg, mode=\"valid\")\n    sampler= DistributedSampler(\n        valid_ds, \n        num_replicas=cfg.world_size, \n        rank=cfg.local_rank,\n    )\n    valid_dl = torch.utils.data.DataLoader(\n        valid_ds, \n        sampler= sampler,\n        batch_size= cfg.batch_size_val, \n        num_workers= 4,\n    )\n\n    # ========== Model / Optim ==========\n    model = Net(backbone=cfg.backbone)\n    model= model.to(cfg.local_rank)\n    if cfg.ema:\n        if cfg.local_rank == 0:\n            print(\"Initializing EMA model..\")\n        ema_model = ModelEMA(\n            model, \n            decay=cfg.ema_decay, \n            device=cfg.local_rank,\n        )\n    else:\n        ema_model = None\n    model= DistributedDataParallel(\n        model, \n        device_ids=[cfg.local_rank], \n        )\n    \n    # üöÄ ENHANCED LOSS FUNCTION\n    criterion = L1_SSIM_Loss(alpha=0.85)\n    \n    # Better optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.epochs)\n    \n    scaler = GradScaler()\n\n    if cfg.local_rank == 0:\n        print(f\"üéØ Loss Function: L1+SSIM (Œ±={criterion.alpha})\")\n        print(f\"üìä Optimizer: AdamW (lr=2e-4, wd=1e-4)\")\n        print(f\"üìà Scheduler: CosineAnnealingLR\")\n        print(f\"üîÑ EMA Decay: {cfg.ema_decay}\")\n        print(\"=\"*50)\n\n    # ========== Training ==========\n    if cfg.local_rank == 0:\n        print(\"üöÄ Starting Enhanced Training...\")\n        print(\"=\"*50)\n    \n    best_loss= 1_000_000\n    val_loss= 1_000_000\n\n    for epoch in range(0, cfg.epochs+1):\n        if epoch != 0:\n            tstart= time.time()\n            train_dl.sampler.set_epoch(epoch)\n    \n            # Train loop\n            model.train()\n            total_loss = []\n            for i, (x, y) in enumerate(train_dl):\n                x = x.to(cfg.local_rank)\n                y = y.to(cfg.local_rank)\n        \n                with autocast(cfg.device.type):\n                    logits = model(x)\n                    \n                loss = criterion(logits, y)\n        \n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n        \n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n    \n                total_loss.append(loss.item())\n                \n                if ema_model is not None:\n                    ema_model.update(model)\n                    \n                if cfg.local_rank == 0 and (len(total_loss) >= cfg.logging_steps or i == 0):\n                    train_loss = np.mean(total_loss)\n                    total_loss = []\n                    current_lr = optimizer.param_groups[0]['lr']\n                    print(\"Epoch {:2d}: Loss={:.4f} | Val MAE={:.2f} | LR={:.2e} | Time={} | Step={:4d}/{}\".format(\n                        epoch, \n                        train_loss,\n                        val_loss,\n                        current_lr,\n                        format_time(time.time() - tstart),\n                        i+1, \n                        len(train_dl), \n                    ))\n            \n            scheduler.step()\n    \n        # ========== Validation ==========\n        model.eval()\n        val_logits = []\n        val_targets = []\n        with torch.no_grad():\n            for x, y in tqdm(valid_dl, disable=cfg.local_rank != 0, desc=f\"Epoch {epoch} Validation\"):\n                x = x.to(cfg.local_rank)\n                y = y.to(cfg.local_rank)\n    \n                with autocast(cfg.device.type):\n                    if ema_model is not None:\n                        out = ema_model.module(x)\n                    else:\n                        out = model(x)\n\n                val_logits.append(out.cpu())\n                val_targets.append(y.cpu())\n\n            val_logits= torch.cat(val_logits, dim=0)\n            val_targets= torch.cat(val_targets, dim=0)\n                \n            # Use L1 for validation metric (competition metric)\n            l1_criterion = nn.L1Loss()\n            loss = l1_criterion(val_logits, val_targets).item()\n\n        # Gather loss\n        v = torch.tensor([loss], device=cfg.local_rank)\n        torch.distributed.all_reduce(v, op=dist.ReduceOp.SUM)\n        val_loss = (v[0] / cfg.world_size).item()\n    \n        # ========== Save Models ==========\n        stop_train = torch.tensor([0], device=cfg.local_rank)\n        if cfg.local_rank == 0:\n            es= cfg.early_stopping\n            if val_loss < best_loss:\n                improvement = best_loss - val_loss\n                print(f\"üéâ NEW BEST: {best_loss:.3f} ‚Üí {val_loss:.3f} (‚Üì{improvement:.3f})\")\n                print(\"üíæ Saved best model...\")\n                best_loss = val_loss\n                if ema_model is not None:\n                    torch.save(ema_model.module.state_dict(), f'enhanced_model_best_{cfg.seed}.pt')\n                else:\n                    torch.save(model.module.state_dict(), f'enhanced_model_best_{cfg.seed}.pt')\n        \n                es[\"streak\"] = 0\n            else:\n                es[\"streak\"] += 1\n                if es[\"streak\"] > es[\"patience\"]:\n                    print(f\"‚èπÔ∏è Early stopping after {es['streak']} epochs without improvement\")\n                    stop_train = torch.tensor([1], device=cfg.local_rank)\n        \n        # Exits training on all ranks\n        dist.broadcast(stop_train, src=0)\n        if stop_train.item() == 1:\n            break\n\n    # Final training summary\n    if cfg.local_rank == 0:\n        print(\"=\"*50)\n        print(\"üèÅ TRAINING COMPLETED!\")\n        print(f\"üèÜ Best Validation MAE: {best_loss:.3f}\")\n        print(f\"üìä Total Epochs: {epoch}\")\n        print(\"=\"*50)\n\n    return\n\n\nif __name__ == \"__main__\":\n\n   # GPU Specs\n   rank = int(os.environ[\"RANK\"])\n   world_size = int(os.environ[\"WORLD_SIZE\"])\n   _, total = torch.cuda.mem_get_info(device=rank)\n\n   # Init\n   setup(rank, world_size)\n   time.sleep(rank)\n   print(f\"Rank: {rank}, World size: {world_size}, GPU memory: {total / 1024**3:.2f}GB\", flush=True)\n   time.sleep(world_size - rank)\n\n   # Seed\n   set_seed(cfg.seed+rank)\n\n   # Run\n   cfg.local_rank= rank\n   cfg.world_size= world_size\n   main(cfg)\n   cleanup()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:40:30.793141Z","iopub.execute_input":"2025-05-23T03:40:30.793335Z","iopub.status.idle":"2025-05-23T03:40:30.807918Z","shell.execute_reply.started":"2025-05-23T03:40:30.793320Z","shell.execute_reply":"2025-05-23T03:40:30.807343Z"}},"outputs":[{"name":"stdout","text":"Overwriting _train.py\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from _cfg import cfg\n\nif RUN_TRAIN:\n    print(\"üöÄ Starting Enhanced Training...\")\n    print(\"=\"*60)\n    print(\"üìã Training Configuration:\")\n    print(f\"   ‚Ä¢ Epochs: {cfg.epochs}\")\n    print(f\"   ‚Ä¢ Batch Size: {cfg.batch_size}\")\n    print(f\"   ‚Ä¢ Loss: L1+SSIM (Œ±=0.85)\")\n    print(f\"   ‚Ä¢ Optimizer: AdamW\")\n    print(f\"   ‚Ä¢ EMA: {cfg.ema}\")\n    print(\"=\"*60)\n    \n    !OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 _train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-23T03:40:30.808727Z","iopub.execute_input":"2025-05-23T03:40:30.809121Z"}},"outputs":[{"name":"stdout","text":"üöÄ Starting Enhanced Training...\n============================================================\nüìã Training Configuration:\n   ‚Ä¢ Epochs: 50\n   ‚Ä¢ Batch Size: 16\n   ‚Ä¢ Loss: L1+SSIM (Œ±=0.85)\n   ‚Ä¢ Optimizer: AdamW\n   ‚Ä¢ EMA: True\n============================================================\n<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-05-23 03:40:43.200099: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-05-23 03:40:43.200759: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747971643.222256     149 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747971643.223961     150 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747971643.229520     149 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1747971643.231724     150 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nRank: 0, World size: 2, GPU memory: 14.74GB\nRank: 1, World size: 2, GPU memory: 14.74GB\n==================================================\nüî• ENHANCED TRAINING WITH L1+SSIM LOSS\n==================================================\nLoading data..\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 920/920 [00:25<00:00, 36.36it/s]\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 20.47it/s]\nmodel.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201M/201M [00:00<00:00, 221MB/s]\nReplacing all activations with GELU...\nReplacing all norms with InstanceNorm...\nReplacing forward functions...\nReplacing all activations with GELU...\nReplacing all norms with InstanceNorm...\nReplacing forward functions...\nInitializing EMA model..\nüéØ Loss Function: L1+SSIM (Œ±=0.85)\nüìä Optimizer: AdamW (lr=2e-4, wd=1e-4)\nüìà Scheduler: CosineAnnealingLR\nüîÑ EMA Decay: 0.999\n==================================================\nüöÄ Starting Enhanced Training...\n==================================================\nEpoch 0 Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [00:58<00:00,  5.33it/s]\nüéâ NEW BEST: 1000000.000 ‚Üí 808.616 (‚Üì999191.384)\nüíæ Saved best model...\nEpoch  1: Loss=703.5829 | Val MAE=808.62 | LR=2.00e-04 | Time=0:00:02 | Step=   1/14375\nEpoch  1: Loss=497.4052 | Val MAE=808.62 | LR=2.00e-04 | Time=0:00:21 | Step=  51/14375\nEpoch  1: Loss=357.4626 | Val MAE=808.62 | LR=2.00e-04 | Time=0:00:40 | Step= 101/14375\nEpoch  1: Loss=350.4845 | Val MAE=808.62 | LR=2.00e-04 | Time=0:01:00 | Step= 151/14375\nEpoch  1: Loss=312.3755 | Val MAE=808.62 | LR=2.00e-04 | Time=0:01:20 | Step= 201/14375\nEpoch  1: Loss=289.5671 | Val MAE=808.62 | LR=2.00e-04 | Time=0:01:39 | Step= 251/14375\nEpoch  1: Loss=293.3904 | Val MAE=808.62 | LR=2.00e-04 | Time=0:01:58 | Step= 301/14375\nEpoch  1: Loss=283.5746 | Val MAE=808.62 | LR=2.00e-04 | Time=0:02:17 | Step= 351/14375\nEpoch  1: Loss=276.1656 | Val MAE=808.62 | LR=2.00e-04 | Time=0:02:37 | Step= 401/14375\nEpoch  1: Loss=272.3396 | Val MAE=808.62 | LR=2.00e-04 | Time=0:02:56 | Step= 451/14375\nEpoch  1: Loss=269.1743 | Val MAE=808.62 | LR=2.00e-04 | Time=0:03:15 | Step= 501/14375\nEpoch  1: Loss=247.0771 | Val MAE=808.62 | LR=2.00e-04 | Time=0:03:35 | Step= 551/14375\nEpoch  1: Loss=254.9700 | Val MAE=808.62 | LR=2.00e-04 | Time=0:03:58 | Step= 601/14375\nEpoch  1: Loss=243.6608 | Val MAE=808.62 | LR=2.00e-04 | Time=0:04:18 | Step= 651/14375\nEpoch  1: Loss=235.2969 | Val MAE=808.62 | LR=2.00e-04 | Time=0:04:38 | Step= 701/14375\nEpoch  1: Loss=237.0040 | Val MAE=808.62 | LR=2.00e-04 | Time=0:04:57 | Step= 751/14375\nEpoch  1: Loss=239.5279 | Val MAE=808.62 | LR=2.00e-04 | Time=0:05:17 | Step= 801/14375\nEpoch  1: Loss=231.6980 | Val MAE=808.62 | LR=2.00e-04 | Time=0:05:36 | Step= 851/14375\nEpoch  1: Loss=219.9328 | Val MAE=808.62 | LR=2.00e-04 | Time=0:05:56 | Step= 901/14375\nEpoch  1: Loss=229.0541 | Val MAE=808.62 | LR=2.00e-04 | Time=0:06:15 | Step= 951/14375\nEpoch  1: Loss=224.0607 | Val MAE=808.62 | LR=2.00e-04 | Time=0:06:35 | Step=1001/14375\nEpoch  1: Loss=208.6228 | Val MAE=808.62 | LR=2.00e-04 | Time=0:06:54 | Step=1051/14375\nEpoch  1: Loss=235.0071 | Val MAE=808.62 | LR=2.00e-04 | Time=0:07:14 | Step=1101/14375\nEpoch  1: Loss=211.3646 | Val MAE=808.62 | LR=2.00e-04 | Time=0:07:34 | Step=1151/14375\nEpoch  1: Loss=208.7357 | Val MAE=808.62 | LR=2.00e-04 | Time=0:07:53 | Step=1201/14375\nEpoch  1: Loss=199.0795 | Val MAE=808.62 | LR=2.00e-04 | Time=0:08:13 | Step=1251/14375\nEpoch  1: Loss=201.0011 | Val MAE=808.62 | LR=2.00e-04 | Time=0:08:32 | Step=1301/14375\nEpoch  1: Loss=184.6646 | Val MAE=808.62 | LR=2.00e-04 | Time=0:08:52 | Step=1351/14375\nEpoch  1: Loss=188.7807 | Val MAE=808.62 | LR=2.00e-04 | Time=0:09:12 | Step=1401/14375\nEpoch  1: Loss=184.7776 | Val MAE=808.62 | LR=2.00e-04 | Time=0:09:31 | Step=1451/14375\nEpoch  1: Loss=178.1197 | Val MAE=808.62 | LR=2.00e-04 | Time=0:09:51 | Step=1501/14375\nEpoch  1: Loss=190.4531 | Val MAE=808.62 | LR=2.00e-04 | Time=0:10:11 | Step=1551/14375\nEpoch  1: Loss=183.4722 | Val MAE=808.62 | LR=2.00e-04 | Time=0:10:30 | Step=1601/14375\nEpoch  1: Loss=172.3677 | Val MAE=808.62 | LR=2.00e-04 | Time=0:10:51 | Step=1651/14375\nEpoch  1: Loss=175.8058 | Val MAE=808.62 | LR=2.00e-04 | Time=0:11:11 | Step=1701/14375\nEpoch  1: Loss=180.5259 | Val MAE=808.62 | LR=2.00e-04 | Time=0:11:30 | Step=1751/14375\nEpoch  1: Loss=176.7415 | Val MAE=808.62 | LR=2.00e-04 | Time=0:11:50 | Step=1801/14375\nEpoch  1: Loss=173.2006 | Val MAE=808.62 | LR=2.00e-04 | Time=0:12:10 | Step=1851/14375\nEpoch  1: Loss=163.4390 | Val MAE=808.62 | LR=2.00e-04 | Time=0:12:30 | Step=1901/14375\nEpoch  1: Loss=170.8318 | Val MAE=808.62 | LR=2.00e-04 | Time=0:12:49 | Step=1951/14375\nEpoch  1: Loss=165.0086 | Val MAE=808.62 | LR=2.00e-04 | Time=0:13:09 | Step=2001/14375\nEpoch  1: Loss=171.0318 | Val MAE=808.62 | LR=2.00e-04 | Time=0:13:29 | Step=2051/14375\nEpoch  1: Loss=182.7724 | Val MAE=808.62 | LR=2.00e-04 | Time=0:13:49 | Step=2101/14375\nEpoch  1: Loss=165.2622 | Val MAE=808.62 | LR=2.00e-04 | Time=0:14:08 | Step=2151/14375\nEpoch  1: Loss=165.2013 | Val MAE=808.62 | LR=2.00e-04 | Time=0:14:28 | Step=2201/14375\nEpoch  1: Loss=153.7270 | Val MAE=808.62 | LR=2.00e-04 | Time=0:14:48 | Step=2251/14375\nEpoch  1: Loss=162.6320 | Val MAE=808.62 | LR=2.00e-04 | Time=0:15:08 | Step=2301/14375\nEpoch  1: Loss=160.5341 | Val MAE=808.62 | LR=2.00e-04 | Time=0:15:28 | Step=2351/14375\nEpoch  1: Loss=148.4305 | Val MAE=808.62 | LR=2.00e-04 | Time=0:15:47 | Step=2401/14375\nEpoch  1: Loss=149.8317 | Val MAE=808.62 | LR=2.00e-04 | Time=0:16:07 | Step=2451/14375\nEpoch  1: Loss=151.1462 | Val MAE=808.62 | LR=2.00e-04 | Time=0:16:27 | Step=2501/14375\nEpoch  1: Loss=149.5322 | Val MAE=808.62 | LR=2.00e-04 | Time=0:16:47 | Step=2551/14375\nEpoch  1: Loss=152.5638 | Val MAE=808.62 | LR=2.00e-04 | Time=0:17:06 | Step=2601/14375\nEpoch  1: Loss=167.1085 | Val MAE=808.62 | LR=2.00e-04 | Time=0:17:26 | Step=2651/14375\nEpoch  1: Loss=180.6951 | Val MAE=808.62 | LR=2.00e-04 | Time=0:17:46 | Step=2701/14375\nEpoch  1: Loss=155.9728 | Val MAE=808.62 | LR=2.00e-04 | Time=0:18:06 | Step=2751/14375\nEpoch  1: Loss=147.9654 | Val MAE=808.62 | LR=2.00e-04 | Time=0:18:25 | Step=2801/14375\nEpoch  1: Loss=144.8044 | Val MAE=808.62 | LR=2.00e-04 | Time=0:18:45 | Step=2851/14375\nEpoch  1: Loss=152.1086 | Val MAE=808.62 | LR=2.00e-04 | Time=0:19:05 | Step=2901/14375\nEpoch  1: Loss=149.2100 | Val MAE=808.62 | LR=2.00e-04 | Time=0:19:25 | Step=2951/14375\nEpoch  1: Loss=141.3621 | Val MAE=808.62 | LR=2.00e-04 | Time=0:19:44 | Step=3001/14375\nEpoch  1: Loss=140.3677 | Val MAE=808.62 | LR=2.00e-04 | Time=0:20:04 | Step=3051/14375\nEpoch  1: Loss=146.2065 | Val MAE=808.62 | LR=2.00e-04 | Time=0:20:24 | Step=3101/14375\nEpoch  1: Loss=145.6115 | Val MAE=808.62 | LR=2.00e-04 | Time=0:20:44 | Step=3151/14375\nEpoch  1: Loss=155.7995 | Val MAE=808.62 | LR=2.00e-04 | Time=0:21:03 | Step=3201/14375\nEpoch  1: Loss=147.7923 | Val MAE=808.62 | LR=2.00e-04 | Time=0:21:23 | Step=3251/14375\nEpoch  1: Loss=143.4695 | Val MAE=808.62 | LR=2.00e-04 | Time=0:21:43 | Step=3301/14375\nEpoch  1: Loss=141.0061 | Val MAE=808.62 | LR=2.00e-04 | Time=0:22:02 | Step=3351/14375\nEpoch  1: Loss=134.1857 | Val MAE=808.62 | LR=2.00e-04 | Time=0:22:22 | Step=3401/14375\nEpoch  1: Loss=141.1946 | Val MAE=808.62 | LR=2.00e-04 | Time=0:22:42 | Step=3451/14375\nEpoch  1: Loss=130.6813 | Val MAE=808.62 | LR=2.00e-04 | Time=0:23:01 | Step=3501/14375\nEpoch  1: Loss=135.5742 | Val MAE=808.62 | LR=2.00e-04 | Time=0:23:21 | Step=3551/14375\nEpoch  1: Loss=130.9661 | Val MAE=808.62 | LR=2.00e-04 | Time=0:23:41 | Step=3601/14375\nEpoch  1: Loss=131.5341 | Val MAE=808.62 | LR=2.00e-04 | Time=0:24:01 | Step=3651/14375\nEpoch  1: Loss=136.5643 | Val MAE=808.62 | LR=2.00e-04 | Time=0:24:21 | Step=3701/14375\nEpoch  1: Loss=128.4682 | Val MAE=808.62 | LR=2.00e-04 | Time=0:24:40 | Step=3751/14375\nEpoch  1: Loss=137.7291 | Val MAE=808.62 | LR=2.00e-04 | Time=0:25:00 | Step=3801/14375\nEpoch  1: Loss=136.2691 | Val MAE=808.62 | LR=2.00e-04 | Time=0:25:20 | Step=3851/14375\nEpoch  1: Loss=131.4549 | Val MAE=808.62 | LR=2.00e-04 | Time=0:25:39 | Step=3901/14375\nEpoch  1: Loss=136.1755 | Val MAE=808.62 | LR=2.00e-04 | Time=0:25:59 | Step=3951/14375\nEpoch  1: Loss=133.4239 | Val MAE=808.62 | LR=2.00e-04 | Time=0:26:19 | Step=4001/14375\nEpoch  1: Loss=134.0340 | Val MAE=808.62 | LR=2.00e-04 | Time=0:26:39 | Step=4051/14375\nEpoch  1: Loss=140.7137 | Val MAE=808.62 | LR=2.00e-04 | Time=0:26:58 | Step=4101/14375\nEpoch  1: Loss=127.9758 | Val MAE=808.62 | LR=2.00e-04 | Time=0:27:18 | Step=4151/14375\nEpoch  1: Loss=135.5970 | Val MAE=808.62 | LR=2.00e-04 | Time=0:27:38 | Step=4201/14375\nEpoch  1: Loss=147.2673 | Val MAE=808.62 | LR=2.00e-04 | Time=0:27:57 | Step=4251/14375\nEpoch  1: Loss=138.7668 | Val MAE=808.62 | LR=2.00e-04 | Time=0:28:17 | Step=4301/14375\nEpoch  1: Loss=125.5062 | Val MAE=808.62 | LR=2.00e-04 | Time=0:28:37 | Step=4351/14375\nEpoch  1: Loss=127.7841 | Val MAE=808.62 | LR=2.00e-04 | Time=0:28:57 | Step=4401/14375\nEpoch  1: Loss=134.7108 | Val MAE=808.62 | LR=2.00e-04 | Time=0:29:17 | Step=4451/14375\nEpoch  1: Loss=135.0589 | Val MAE=808.62 | LR=2.00e-04 | Time=0:29:36 | Step=4501/14375\nEpoch  1: Loss=125.2047 | Val MAE=808.62 | LR=2.00e-04 | Time=0:29:56 | Step=4551/14375\nEpoch  1: Loss=119.1464 | Val MAE=808.62 | LR=2.00e-04 | Time=0:30:16 | Step=4601/14375\nEpoch  1: Loss=121.3979 | Val MAE=808.62 | LR=2.00e-04 | Time=0:30:35 | Step=4651/14375\nEpoch  1: Loss=127.7064 | Val MAE=808.62 | LR=2.00e-04 | Time=0:30:55 | Step=4701/14375\nEpoch  1: Loss=122.8486 | Val MAE=808.62 | LR=2.00e-04 | Time=0:31:15 | Step=4751/14375\nEpoch  1: Loss=120.7358 | Val MAE=808.62 | LR=2.00e-04 | Time=0:31:36 | Step=4801/14375\nEpoch  1: Loss=121.6988 | Val MAE=808.62 | LR=2.00e-04 | Time=0:31:55 | Step=4851/14375\nEpoch  1: Loss=129.4762 | Val MAE=808.62 | LR=2.00e-04 | Time=0:32:15 | Step=4901/14375\nEpoch  1: Loss=120.8259 | Val MAE=808.62 | LR=2.00e-04 | Time=0:32:35 | Step=4951/14375\nEpoch  1: Loss=115.8029 | Val MAE=808.62 | LR=2.00e-04 | Time=0:32:55 | Step=5001/14375\nEpoch  1: Loss=120.1968 | Val MAE=808.62 | LR=2.00e-04 | Time=0:33:14 | Step=5051/14375\nEpoch  1: Loss=120.8285 | Val MAE=808.62 | LR=2.00e-04 | Time=0:33:34 | Step=5101/14375\nEpoch  1: Loss=122.8902 | Val MAE=808.62 | LR=2.00e-04 | Time=0:33:54 | Step=5151/14375\nEpoch  1: Loss=121.4382 | Val MAE=808.62 | LR=2.00e-04 | Time=0:34:13 | Step=5201/14375\nEpoch  1: Loss=122.4090 | Val MAE=808.62 | LR=2.00e-04 | Time=0:34:33 | Step=5251/14375\nEpoch  1: Loss=120.8513 | Val MAE=808.62 | LR=2.00e-04 | Time=0:34:53 | Step=5301/14375\nEpoch  1: Loss=120.9385 | Val MAE=808.62 | LR=2.00e-04 | Time=0:35:13 | Step=5351/14375\nEpoch  1: Loss=122.3800 | Val MAE=808.62 | LR=2.00e-04 | Time=0:35:32 | Step=5401/14375\nEpoch  1: Loss=123.7252 | Val MAE=808.62 | LR=2.00e-04 | Time=0:35:52 | Step=5451/14375\nEpoch  1: Loss=125.6534 | Val MAE=808.62 | LR=2.00e-04 | Time=0:36:12 | Step=5501/14375\nEpoch  1: Loss=123.0089 | Val MAE=808.62 | LR=2.00e-04 | Time=0:36:32 | Step=5551/14375\nEpoch  1: Loss=117.9258 | Val MAE=808.62 | LR=2.00e-04 | Time=0:36:52 | Step=5601/14375\nEpoch  1: Loss=114.3781 | Val MAE=808.62 | LR=2.00e-04 | Time=0:37:11 | Step=5651/14375\nEpoch  1: Loss=115.4663 | Val MAE=808.62 | LR=2.00e-04 | Time=0:37:31 | Step=5701/14375\nEpoch  1: Loss=118.5841 | Val MAE=808.62 | LR=2.00e-04 | Time=0:37:50 | Step=5751/14375\nEpoch  1: Loss=118.8629 | Val MAE=808.62 | LR=2.00e-04 | Time=0:38:10 | Step=5801/14375\nEpoch  1: Loss=127.5014 | Val MAE=808.62 | LR=2.00e-04 | Time=0:38:29 | Step=5851/14375\nEpoch  1: Loss=117.0719 | Val MAE=808.62 | LR=2.00e-04 | Time=0:38:49 | Step=5901/14375\nEpoch  1: Loss=125.2207 | Val MAE=808.62 | LR=2.00e-04 | Time=0:39:08 | Step=5951/14375\nEpoch  1: Loss=115.0232 | Val MAE=808.62 | LR=2.00e-04 | Time=0:39:28 | Step=6001/14375\nEpoch  1: Loss=112.2407 | Val MAE=808.62 | LR=2.00e-04 | Time=0:39:47 | Step=6051/14375\nEpoch  1: Loss=119.6782 | Val MAE=808.62 | LR=2.00e-04 | Time=0:40:06 | Step=6101/14375\nEpoch  1: Loss=130.2654 | Val MAE=808.62 | LR=2.00e-04 | Time=0:40:26 | Step=6151/14375\nEpoch  1: Loss=110.0625 | Val MAE=808.62 | LR=2.00e-04 | Time=0:40:45 | Step=6201/14375\nEpoch  1: Loss=113.7648 | Val MAE=808.62 | LR=2.00e-04 | Time=0:41:05 | Step=6251/14375\nEpoch  1: Loss=115.4540 | Val MAE=808.62 | LR=2.00e-04 | Time=0:41:24 | Step=6301/14375\nEpoch  1: Loss=122.9375 | Val MAE=808.62 | LR=2.00e-04 | Time=0:41:43 | Step=6351/14375\nEpoch  1: Loss=107.1523 | Val MAE=808.62 | LR=2.00e-04 | Time=0:42:03 | Step=6401/14375\nEpoch  1: Loss=105.8036 | Val MAE=808.62 | LR=2.00e-04 | Time=0:42:22 | Step=6451/14375\nEpoch  1: Loss=108.8040 | Val MAE=808.62 | LR=2.00e-04 | Time=0:42:42 | Step=6501/14375\nEpoch  1: Loss=107.4580 | Val MAE=808.62 | LR=2.00e-04 | Time=0:43:01 | Step=6551/14375\nEpoch  1: Loss=111.5785 | Val MAE=808.62 | LR=2.00e-04 | Time=0:43:21 | Step=6601/14375\nEpoch  1: Loss=108.6620 | Val MAE=808.62 | LR=2.00e-04 | Time=0:43:40 | Step=6651/14375\nEpoch  1: Loss=107.2345 | Val MAE=808.62 | LR=2.00e-04 | Time=0:43:59 | Step=6701/14375\nEpoch  1: Loss=114.1113 | Val MAE=808.62 | LR=2.00e-04 | Time=0:44:19 | Step=6751/14375\nEpoch  1: Loss=110.5473 | Val MAE=808.62 | LR=2.00e-04 | Time=0:44:38 | Step=6801/14375\nEpoch  1: Loss=107.3155 | Val MAE=808.62 | LR=2.00e-04 | Time=0:44:58 | Step=6851/14375\nEpoch  1: Loss=113.3803 | Val MAE=808.62 | LR=2.00e-04 | Time=0:45:17 | Step=6901/14375\nEpoch  1: Loss=116.1685 | Val MAE=808.62 | LR=2.00e-04 | Time=0:45:37 | Step=6951/14375\nEpoch  1: Loss=111.6780 | Val MAE=808.62 | LR=2.00e-04 | Time=0:45:57 | Step=7001/14375\nEpoch  1: Loss=109.8020 | Val MAE=808.62 | LR=2.00e-04 | Time=0:46:16 | Step=7051/14375\nEpoch  1: Loss=111.5660 | Val MAE=808.62 | LR=2.00e-04 | Time=0:46:36 | Step=7101/14375\nEpoch  1: Loss=110.3506 | Val MAE=808.62 | LR=2.00e-04 | Time=0:46:56 | Step=7151/14375\nEpoch  1: Loss=102.2751 | Val MAE=808.62 | LR=2.00e-04 | Time=0:47:15 | Step=7201/14375\nEpoch  1: Loss=105.8742 | Val MAE=808.62 | LR=2.00e-04 | Time=0:47:35 | Step=7251/14375\nEpoch  1: Loss=105.8597 | Val MAE=808.62 | LR=2.00e-04 | Time=0:47:55 | Step=7301/14375\nEpoch  1: Loss=109.0851 | Val MAE=808.62 | LR=2.00e-04 | Time=0:48:14 | Step=7351/14375\nEpoch  1: Loss=118.2542 | Val MAE=808.62 | LR=2.00e-04 | Time=0:48:34 | Step=7401/14375\nEpoch  1: Loss=116.2205 | Val MAE=808.62 | LR=2.00e-04 | Time=0:48:53 | Step=7451/14375\nEpoch  1: Loss=114.5944 | Val MAE=808.62 | LR=2.00e-04 | Time=0:49:13 | Step=7501/14375\nEpoch  1: Loss=108.6509 | Val MAE=808.62 | LR=2.00e-04 | Time=0:49:33 | Step=7551/14375\nEpoch  1: Loss=107.0967 | Val MAE=808.62 | LR=2.00e-04 | Time=0:49:52 | Step=7601/14375\nEpoch  1: Loss=113.7973 | Val MAE=808.62 | LR=2.00e-04 | Time=0:50:12 | Step=7651/14375\nEpoch  1: Loss=111.0527 | Val MAE=808.62 | LR=2.00e-04 | Time=0:50:32 | Step=7701/14375\nEpoch  1: Loss=107.5027 | Val MAE=808.62 | LR=2.00e-04 | Time=0:50:51 | Step=7751/14375\nEpoch  1: Loss=110.1345 | Val MAE=808.62 | LR=2.00e-04 | Time=0:51:11 | Step=7801/14375\nEpoch  1: Loss=109.9684 | Val MAE=808.62 | LR=2.00e-04 | Time=0:51:31 | Step=7851/14375\nEpoch  1: Loss=128.7734 | Val MAE=808.62 | LR=2.00e-04 | Time=0:51:50 | Step=7901/14375\nEpoch  1: Loss=110.7011 | Val MAE=808.62 | LR=2.00e-04 | Time=0:52:10 | Step=7951/14375\nEpoch  1: Loss=106.6257 | Val MAE=808.62 | LR=2.00e-04 | Time=0:52:30 | Step=8001/14375\nEpoch  1: Loss=108.8395 | Val MAE=808.62 | LR=2.00e-04 | Time=0:52:50 | Step=8051/14375\nEpoch  1: Loss=109.7302 | Val MAE=808.62 | LR=2.00e-04 | Time=0:53:09 | Step=8101/14375\nEpoch  1: Loss=106.5155 | Val MAE=808.62 | LR=2.00e-04 | Time=0:53:29 | Step=8151/14375\nEpoch  1: Loss=101.7138 | Val MAE=808.62 | LR=2.00e-04 | Time=0:53:49 | Step=8201/14375\nEpoch  1: Loss=100.5377 | Val MAE=808.62 | LR=2.00e-04 | Time=0:54:08 | Step=8251/14375\nEpoch  1: Loss=103.0838 | Val MAE=808.62 | LR=2.00e-04 | Time=0:54:28 | Step=8301/14375\nEpoch  1: Loss=112.1728 | Val MAE=808.62 | LR=2.00e-04 | Time=0:54:48 | Step=8351/14375\nEpoch  1: Loss=107.1517 | Val MAE=808.62 | LR=2.00e-04 | Time=0:55:07 | Step=8401/14375\nEpoch  1: Loss=105.0491 | Val MAE=808.62 | LR=2.00e-04 | Time=0:55:27 | Step=8451/14375\nEpoch  1: Loss=107.8406 | Val MAE=808.62 | LR=2.00e-04 | Time=0:55:47 | Step=8501/14375\nEpoch  1: Loss=107.2222 | Val MAE=808.62 | LR=2.00e-04 | Time=0:56:06 | Step=8551/14375\nEpoch  1: Loss=107.0990 | Val MAE=808.62 | LR=2.00e-04 | Time=0:56:26 | Step=8601/14375\nEpoch  1: Loss=104.5695 | Val MAE=808.62 | LR=2.00e-04 | Time=0:56:46 | Step=8651/14375\nEpoch  1: Loss=96.4939 | Val MAE=808.62 | LR=2.00e-04 | Time=0:57:05 | Step=8701/14375\nEpoch  1: Loss=105.6018 | Val MAE=808.62 | LR=2.00e-04 | Time=0:57:25 | Step=8751/14375\nEpoch  1: Loss=109.0946 | Val MAE=808.62 | LR=2.00e-04 | Time=0:57:44 | Step=8801/14375\nEpoch  1: Loss=101.8015 | Val MAE=808.62 | LR=2.00e-04 | Time=0:58:04 | Step=8851/14375\nEpoch  1: Loss=100.4310 | Val MAE=808.62 | LR=2.00e-04 | Time=0:58:23 | Step=8901/14375\nEpoch  1: Loss=104.9369 | Val MAE=808.62 | LR=2.00e-04 | Time=0:58:43 | Step=8951/14375\nEpoch  1: Loss=103.0876 | Val MAE=808.62 | LR=2.00e-04 | Time=0:59:02 | Step=9001/14375\nEpoch  1: Loss=97.1704 | Val MAE=808.62 | LR=2.00e-04 | Time=0:59:22 | Step=9051/14375\nEpoch  1: Loss=98.9197 | Val MAE=808.62 | LR=2.00e-04 | Time=0:59:41 | Step=9101/14375\nEpoch  1: Loss=103.8571 | Val MAE=808.62 | LR=2.00e-04 | Time=1:00:01 | Step=9151/14375\nEpoch  1: Loss=95.1139 | Val MAE=808.62 | LR=2.00e-04 | Time=1:00:20 | Step=9201/14375\nEpoch  1: Loss=94.5403 | Val MAE=808.62 | LR=2.00e-04 | Time=1:00:40 | Step=9251/14375\nEpoch  1: Loss=104.6982 | Val MAE=808.62 | LR=2.00e-04 | Time=1:01:00 | Step=9301/14375\nEpoch  1: Loss=104.2845 | Val MAE=808.62 | LR=2.00e-04 | Time=1:01:19 | Step=9351/14375\nEpoch  1: Loss=99.6604 | Val MAE=808.62 | LR=2.00e-04 | Time=1:01:39 | Step=9401/14375\nEpoch  1: Loss=103.3341 | Val MAE=808.62 | LR=2.00e-04 | Time=1:01:59 | Step=9451/14375\nEpoch  1: Loss=98.8277 | Val MAE=808.62 | LR=2.00e-04 | Time=1:02:18 | Step=9501/14375\nEpoch  1: Loss=99.8484 | Val MAE=808.62 | LR=2.00e-04 | Time=1:02:38 | Step=9551/14375\nEpoch  1: Loss=100.0987 | Val MAE=808.62 | LR=2.00e-04 | Time=1:02:58 | Step=9601/14375\nEpoch  1: Loss=100.0096 | Val MAE=808.62 | LR=2.00e-04 | Time=1:03:17 | Step=9651/14375\nEpoch  1: Loss=118.8755 | Val MAE=808.62 | LR=2.00e-04 | Time=1:03:37 | Step=9701/14375\nEpoch  1: Loss=100.1161 | Val MAE=808.62 | LR=2.00e-04 | Time=1:03:56 | Step=9751/14375\nEpoch  1: Loss=96.0524 | Val MAE=808.62 | LR=2.00e-04 | Time=1:04:16 | Step=9801/14375\nEpoch  1: Loss=102.5273 | Val MAE=808.62 | LR=2.00e-04 | Time=1:04:36 | Step=9851/14375\nEpoch  1: Loss=102.2599 | Val MAE=808.62 | LR=2.00e-04 | Time=1:04:55 | Step=9901/14375\nEpoch  1: Loss=100.5609 | Val MAE=808.62 | LR=2.00e-04 | Time=1:05:15 | Step=9951/14375\nEpoch  1: Loss=103.4031 | Val MAE=808.62 | LR=2.00e-04 | Time=1:05:35 | Step=10001/14375\nEpoch  1: Loss=96.6187 | Val MAE=808.62 | LR=2.00e-04 | Time=1:05:54 | Step=10051/14375\nEpoch  1: Loss=95.1778 | Val MAE=808.62 | LR=2.00e-04 | Time=1:06:14 | Step=10101/14375\nEpoch  1: Loss=98.9964 | Val MAE=808.62 | LR=2.00e-04 | Time=1:06:33 | Step=10151/14375\nEpoch  1: Loss=97.7234 | Val MAE=808.62 | LR=2.00e-04 | Time=1:06:53 | Step=10201/14375\nEpoch  1: Loss=92.5512 | Val MAE=808.62 | LR=2.00e-04 | Time=1:07:12 | Step=10251/14375\nEpoch  1: Loss=95.2061 | Val MAE=808.62 | LR=2.00e-04 | Time=1:07:32 | Step=10301/14375\nEpoch  1: Loss=96.3420 | Val MAE=808.62 | LR=2.00e-04 | Time=1:07:52 | Step=10351/14375\nEpoch  1: Loss=98.2510 | Val MAE=808.62 | LR=2.00e-04 | Time=1:08:12 | Step=10401/14375\nEpoch  1: Loss=93.0806 | Val MAE=808.62 | LR=2.00e-04 | Time=1:08:31 | Step=10451/14375\nEpoch  1: Loss=97.4066 | Val MAE=808.62 | LR=2.00e-04 | Time=1:08:51 | Step=10501/14375\nEpoch  1: Loss=103.3996 | Val MAE=808.62 | LR=2.00e-04 | Time=1:09:10 | Step=10551/14375\nEpoch  1: Loss=100.0625 | Val MAE=808.62 | LR=2.00e-04 | Time=1:09:29 | Step=10601/14375\nEpoch  1: Loss=105.2382 | Val MAE=808.62 | LR=2.00e-04 | Time=1:09:49 | Step=10651/14375\nEpoch  1: Loss=96.5457 | Val MAE=808.62 | LR=2.00e-04 | Time=1:10:08 | Step=10701/14375\nEpoch  1: Loss=95.3721 | Val MAE=808.62 | LR=2.00e-04 | Time=1:10:27 | Step=10751/14375\nEpoch  1: Loss=98.5985 | Val MAE=808.62 | LR=2.00e-04 | Time=1:10:47 | Step=10801/14375\nEpoch  1: Loss=94.3072 | Val MAE=808.62 | LR=2.00e-04 | Time=1:11:06 | Step=10851/14375\nEpoch  1: Loss=94.9863 | Val MAE=808.62 | LR=2.00e-04 | Time=1:11:25 | Step=10901/14375\nEpoch  1: Loss=91.7958 | Val MAE=808.62 | LR=2.00e-04 | Time=1:11:45 | Step=10951/14375\nEpoch  1: Loss=102.4298 | Val MAE=808.62 | LR=2.00e-04 | Time=1:12:04 | Step=11001/14375\nEpoch  1: Loss=92.6509 | Val MAE=808.62 | LR=2.00e-04 | Time=1:12:24 | Step=11051/14375\nEpoch  1: Loss=94.4607 | Val MAE=808.62 | LR=2.00e-04 | Time=1:12:43 | Step=11101/14375\nEpoch  1: Loss=101.9243 | Val MAE=808.62 | LR=2.00e-04 | Time=1:13:03 | Step=11151/14375\nEpoch  1: Loss=104.9752 | Val MAE=808.62 | LR=2.00e-04 | Time=1:13:22 | Step=11201/14375\nEpoch  1: Loss=92.3094 | Val MAE=808.62 | LR=2.00e-04 | Time=1:13:42 | Step=11251/14375\nEpoch  1: Loss=92.8019 | Val MAE=808.62 | LR=2.00e-04 | Time=1:14:01 | Step=11301/14375\nEpoch  1: Loss=92.4867 | Val MAE=808.62 | LR=2.00e-04 | Time=1:14:20 | Step=11351/14375\nEpoch  1: Loss=93.6287 | Val MAE=808.62 | LR=2.00e-04 | Time=1:14:39 | Step=11401/14375\nEpoch  1: Loss=93.1211 | Val MAE=808.62 | LR=2.00e-04 | Time=1:14:59 | Step=11451/14375\nEpoch  1: Loss=99.2287 | Val MAE=808.62 | LR=2.00e-04 | Time=1:15:18 | Step=11501/14375\nEpoch  1: Loss=96.7823 | Val MAE=808.62 | LR=2.00e-04 | Time=1:15:37 | Step=11551/14375\nEpoch  1: Loss=101.6970 | Val MAE=808.62 | LR=2.00e-04 | Time=1:15:56 | Step=11601/14375\nEpoch  1: Loss=94.2983 | Val MAE=808.62 | LR=2.00e-04 | Time=1:16:16 | Step=11651/14375\nEpoch  1: Loss=111.9591 | Val MAE=808.62 | LR=2.00e-04 | Time=1:16:35 | Step=11701/14375\nEpoch  1: Loss=95.1379 | Val MAE=808.62 | LR=2.00e-04 | Time=1:16:54 | Step=11751/14375\nEpoch  1: Loss=91.4643 | Val MAE=808.62 | LR=2.00e-04 | Time=1:17:14 | Step=11801/14375\nEpoch  1: Loss=90.0233 | Val MAE=808.62 | LR=2.00e-04 | Time=1:17:33 | Step=11851/14375\nEpoch  1: Loss=94.9142 | Val MAE=808.62 | LR=2.00e-04 | Time=1:17:52 | Step=11901/14375\nEpoch  1: Loss=103.6377 | Val MAE=808.62 | LR=2.00e-04 | Time=1:18:12 | Step=11951/14375\nEpoch  1: Loss=96.3111 | Val MAE=808.62 | LR=2.00e-04 | Time=1:18:31 | Step=12001/14375\nEpoch  1: Loss=92.6898 | Val MAE=808.62 | LR=2.00e-04 | Time=1:18:50 | Step=12051/14375\nEpoch  1: Loss=92.2656 | Val MAE=808.62 | LR=2.00e-04 | Time=1:19:09 | Step=12101/14375\nEpoch  1: Loss=95.1914 | Val MAE=808.62 | LR=2.00e-04 | Time=1:19:29 | Step=12151/14375\nEpoch  1: Loss=88.7474 | Val MAE=808.62 | LR=2.00e-04 | Time=1:19:48 | Step=12201/14375\nEpoch  1: Loss=100.6573 | Val MAE=808.62 | LR=2.00e-04 | Time=1:20:07 | Step=12251/14375\nEpoch  1: Loss=96.3351 | Val MAE=808.62 | LR=2.00e-04 | Time=1:20:27 | Step=12301/14375\nEpoch  1: Loss=91.3531 | Val MAE=808.62 | LR=2.00e-04 | Time=1:20:46 | Step=12351/14375\nEpoch  1: Loss=96.4787 | Val MAE=808.62 | LR=2.00e-04 | Time=1:21:08 | Step=12401/14375\nEpoch  1: Loss=90.5869 | Val MAE=808.62 | LR=2.00e-04 | Time=1:21:27 | Step=12451/14375\nEpoch  1: Loss=103.3395 | Val MAE=808.62 | LR=2.00e-04 | Time=1:21:47 | Step=12501/14375\nEpoch  1: Loss=93.4057 | Val MAE=808.62 | LR=2.00e-04 | Time=1:22:06 | Step=12551/14375\nEpoch  1: Loss=91.4887 | Val MAE=808.62 | LR=2.00e-04 | Time=1:22:25 | Step=12601/14375\nEpoch  1: Loss=90.3241 | Val MAE=808.62 | LR=2.00e-04 | Time=1:22:44 | Step=12651/14375\nEpoch  1: Loss=90.1065 | Val MAE=808.62 | LR=2.00e-04 | Time=1:23:04 | Step=12701/14375\nEpoch  1: Loss=89.4524 | Val MAE=808.62 | LR=2.00e-04 | Time=1:23:23 | Step=12751/14375\nEpoch  1: Loss=85.9255 | Val MAE=808.62 | LR=2.00e-04 | Time=1:23:42 | Step=12801/14375\nEpoch  1: Loss=88.6791 | Val MAE=808.62 | LR=2.00e-04 | Time=1:24:01 | Step=12851/14375\nEpoch  1: Loss=102.5334 | Val MAE=808.62 | LR=2.00e-04 | Time=1:24:21 | Step=12901/14375\nEpoch  1: Loss=103.1996 | Val MAE=808.62 | LR=2.00e-04 | Time=1:24:40 | Step=12951/14375\nEpoch  1: Loss=93.5942 | Val MAE=808.62 | LR=2.00e-04 | Time=1:25:00 | Step=13001/14375\nEpoch  1: Loss=93.8548 | Val MAE=808.62 | LR=2.00e-04 | Time=1:25:19 | Step=13051/14375\nEpoch  1: Loss=97.7354 | Val MAE=808.62 | LR=2.00e-04 | Time=1:25:38 | Step=13101/14375\nEpoch  1: Loss=100.1122 | Val MAE=808.62 | LR=2.00e-04 | Time=1:25:58 | Step=13151/14375\nEpoch  1: Loss=96.5624 | Val MAE=808.62 | LR=2.00e-04 | Time=1:26:17 | Step=13201/14375\nEpoch  1: Loss=92.4303 | Val MAE=808.62 | LR=2.00e-04 | Time=1:26:37 | Step=13251/14375\nEpoch  1: Loss=87.5068 | Val MAE=808.62 | LR=2.00e-04 | Time=1:26:56 | Step=13301/14375\nEpoch  1: Loss=82.2453 | Val MAE=808.62 | LR=2.00e-04 | Time=1:27:15 | Step=13351/14375\nEpoch  1: Loss=91.2701 | Val MAE=808.62 | LR=2.00e-04 | Time=1:27:35 | Step=13401/14375\nEpoch  1: Loss=96.1170 | Val MAE=808.62 | LR=2.00e-04 | Time=1:27:54 | Step=13451/14375\nEpoch  1: Loss=93.2318 | Val MAE=808.62 | LR=2.00e-04 | Time=1:28:13 | Step=13501/14375\nEpoch  1: Loss=87.1290 | Val MAE=808.62 | LR=2.00e-04 | Time=1:28:33 | Step=13551/14375\nEpoch  1: Loss=87.6174 | Val MAE=808.62 | LR=2.00e-04 | Time=1:28:52 | Step=13601/14375\nEpoch  1: Loss=89.7011 | Val MAE=808.62 | LR=2.00e-04 | Time=1:29:12 | Step=13651/14375\nEpoch  1: Loss=87.7603 | Val MAE=808.62 | LR=2.00e-04 | Time=1:29:31 | Step=13701/14375\nEpoch  1: Loss=87.1299 | Val MAE=808.62 | LR=2.00e-04 | Time=1:29:50 | Step=13751/14375\nEpoch  1: Loss=90.5861 | Val MAE=808.62 | LR=2.00e-04 | Time=1:30:10 | Step=13801/14375\nEpoch  1: Loss=93.0616 | Val MAE=808.62 | LR=2.00e-04 | Time=1:30:29 | Step=13851/14375\nEpoch  1: Loss=90.1515 | Val MAE=808.62 | LR=2.00e-04 | Time=1:30:48 | Step=13901/14375\nEpoch  1: Loss=96.1630 | Val MAE=808.62 | LR=2.00e-04 | Time=1:31:08 | Step=13951/14375\nEpoch  1: Loss=96.5543 | Val MAE=808.62 | LR=2.00e-04 | Time=1:31:27 | Step=14001/14375\nEpoch  1: Loss=93.2245 | Val MAE=808.62 | LR=2.00e-04 | Time=1:31:47 | Step=14051/14375\nEpoch  1: Loss=86.7389 | Val MAE=808.62 | LR=2.00e-04 | Time=1:32:06 | Step=14101/14375\nEpoch  1: Loss=87.1811 | Val MAE=808.62 | LR=2.00e-04 | Time=1:32:26 | Step=14151/14375\nEpoch  1: Loss=90.6074 | Val MAE=808.62 | LR=2.00e-04 | Time=1:32:45 | Step=14201/14375\nEpoch  1: Loss=86.7859 | Val MAE=808.62 | LR=2.00e-04 | Time=1:33:04 | Step=14251/14375\nEpoch  1: Loss=88.4297 | Val MAE=808.62 | LR=2.00e-04 | Time=1:33:24 | Step=14301/14375\nEpoch  1: Loss=88.5078 | Val MAE=808.62 | LR=2.00e-04 | Time=1:33:43 | Step=14351/14375\nEpoch 1 Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 313/313 [01:02<00:00,  5.00it/s]\nüéâ NEW BEST: 808.616 ‚Üí 84.700 (‚Üì723.915)\nüíæ Saved best model...\nEpoch  2: Loss=68.8583 | Val MAE=84.70 | LR=2.00e-04 | Time=0:00:01 | Step=   1/14375\nEpoch  2: Loss=86.6374 | Val MAE=84.70 | LR=2.00e-04 | Time=0:00:20 | Step=  51/14375\nEpoch  2: Loss=89.8932 | Val MAE=84.70 | LR=2.00e-04 | Time=0:00:40 | Step= 101/14375\nEpoch  2: Loss=92.0734 | Val MAE=84.70 | LR=2.00e-04 | Time=0:00:59 | Step= 151/14375\nEpoch  2: Loss=92.0062 | Val MAE=84.70 | LR=2.00e-04 | Time=0:01:18 | Step= 201/14375\nEpoch  2: Loss=91.8499 | Val MAE=84.70 | LR=2.00e-04 | Time=0:01:38 | Step= 251/14375\nEpoch  2: Loss=92.0468 | Val MAE=84.70 | LR=2.00e-04 | Time=0:01:57 | Step= 301/14375\nEpoch  2: Loss=87.7051 | Val MAE=84.70 | LR=2.00e-04 | Time=0:02:16 | Step= 351/14375\nEpoch  2: Loss=83.9111 | Val MAE=84.70 | LR=2.00e-04 | Time=0:02:36 | Step= 401/14375\nEpoch  2: Loss=97.5340 | Val MAE=84.70 | LR=2.00e-04 | Time=0:02:55 | Step= 451/14375\nEpoch  2: Loss=91.7858 | Val MAE=84.70 | LR=2.00e-04 | Time=0:03:14 | Step= 501/14375\nEpoch  2: Loss=88.7641 | Val MAE=84.70 | LR=2.00e-04 | Time=0:03:34 | Step= 551/14375\nEpoch  2: Loss=86.8355 | Val MAE=84.70 | LR=2.00e-04 | Time=0:03:53 | Step= 601/14375\nEpoch  2: Loss=86.9830 | Val MAE=84.70 | LR=2.00e-04 | Time=0:04:13 | Step= 651/14375\nEpoch  2: Loss=87.2779 | Val MAE=84.70 | LR=2.00e-04 | Time=0:04:32 | Step= 701/14375\nEpoch  2: Loss=91.3629 | Val MAE=84.70 | LR=2.00e-04 | Time=0:04:51 | Step= 751/14375\nEpoch  2: Loss=86.0174 | Val MAE=84.70 | LR=2.00e-04 | Time=0:05:11 | Step= 801/14375\nEpoch  2: Loss=84.0667 | Val MAE=84.70 | LR=2.00e-04 | Time=0:05:30 | Step= 851/14375\nEpoch  2: Loss=91.0956 | Val MAE=84.70 | LR=2.00e-04 | Time=0:05:50 | Step= 901/14375\nEpoch  2: Loss=85.7740 | Val MAE=84.70 | LR=2.00e-04 | Time=0:06:09 | Step= 951/14375\nEpoch  2: Loss=82.0938 | Val MAE=84.70 | LR=2.00e-04 | Time=0:06:29 | Step=1001/14375\nEpoch  2: Loss=91.3610 | Val MAE=84.70 | LR=2.00e-04 | Time=0:06:48 | Step=1051/14375\nEpoch  2: Loss=80.5944 | Val MAE=84.70 | LR=2.00e-04 | Time=0:07:08 | Step=1101/14375\nEpoch  2: Loss=83.7735 | Val MAE=84.70 | LR=2.00e-04 | Time=0:07:27 | Step=1151/14375\nEpoch  2: Loss=89.9502 | Val MAE=84.70 | LR=2.00e-04 | Time=0:07:47 | Step=1201/14375\nEpoch  2: Loss=84.9545 | Val MAE=84.70 | LR=2.00e-04 | Time=0:08:06 | Step=1251/14375\nEpoch  2: Loss=90.5160 | Val MAE=84.70 | LR=2.00e-04 | Time=0:08:25 | Step=1301/14375\nEpoch  2: Loss=87.7908 | Val MAE=84.70 | LR=2.00e-04 | Time=0:08:45 | Step=1351/14375\nEpoch  2: Loss=85.4229 | Val MAE=84.70 | LR=2.00e-04 | Time=0:09:04 | Step=1401/14375\nEpoch  2: Loss=78.9238 | Val MAE=84.70 | LR=2.00e-04 | Time=0:09:23 | Step=1451/14375\nEpoch  2: Loss=83.7401 | Val MAE=84.70 | LR=2.00e-04 | Time=0:09:43 | Step=1501/14375\nEpoch  2: Loss=90.9196 | Val MAE=84.70 | LR=2.00e-04 | Time=0:10:02 | Step=1551/14375\nEpoch  2: Loss=90.9947 | Val MAE=84.70 | LR=2.00e-04 | Time=0:10:22 | Step=1601/14375\nEpoch  2: Loss=87.4177 | Val MAE=84.70 | LR=2.00e-04 | Time=0:10:41 | Step=1651/14375\nEpoch  2: Loss=83.5691 | Val MAE=84.70 | LR=2.00e-04 | Time=0:11:00 | Step=1701/14375\nEpoch  2: Loss=88.4921 | Val MAE=84.70 | LR=2.00e-04 | Time=0:11:20 | Step=1751/14375\nEpoch  2: Loss=87.4160 | Val MAE=84.70 | LR=2.00e-04 | Time=0:11:40 | Step=1801/14375\nEpoch  2: Loss=90.3402 | Val MAE=84.70 | LR=2.00e-04 | Time=0:11:59 | Step=1851/14375\nEpoch  2: Loss=93.3869 | Val MAE=84.70 | LR=2.00e-04 | Time=0:12:19 | Step=1901/14375\nEpoch  2: Loss=88.5799 | Val MAE=84.70 | LR=2.00e-04 | Time=0:12:38 | Step=1951/14375\nEpoch  2: Loss=82.6469 | Val MAE=84.70 | LR=2.00e-04 | Time=0:12:57 | Step=2001/14375\nEpoch  2: Loss=82.0211 | Val MAE=84.70 | LR=2.00e-04 | Time=0:13:17 | Step=2051/14375\nEpoch  2: Loss=78.1081 | Val MAE=84.70 | LR=2.00e-04 | Time=0:13:36 | Step=2101/14375\nEpoch  2: Loss=85.4649 | Val MAE=84.70 | LR=2.00e-04 | Time=0:13:55 | Step=2151/14375\nEpoch  2: Loss=89.1024 | Val MAE=84.70 | LR=2.00e-04 | Time=0:14:14 | Step=2201/14375\nEpoch  2: Loss=86.2093 | Val MAE=84.70 | LR=2.00e-04 | Time=0:14:34 | Step=2251/14375\nEpoch  2: Loss=81.9003 | Val MAE=84.70 | LR=2.00e-04 | Time=0:14:53 | Step=2301/14375\nEpoch  2: Loss=82.2716 | Val MAE=84.70 | LR=2.00e-04 | Time=0:15:12 | Step=2351/14375\nEpoch  2: Loss=86.4983 | Val MAE=84.70 | LR=2.00e-04 | Time=0:15:32 | Step=2401/14375\nEpoch  2: Loss=86.3658 | Val MAE=84.70 | LR=2.00e-04 | Time=0:15:51 | Step=2451/14375\nEpoch  2: Loss=82.9961 | Val MAE=84.70 | LR=2.00e-04 | Time=0:16:10 | Step=2501/14375\nEpoch  2: Loss=85.8583 | Val MAE=84.70 | LR=2.00e-04 | Time=0:16:30 | Step=2551/14375\nEpoch  2: Loss=84.4782 | Val MAE=84.70 | LR=2.00e-04 | Time=0:16:49 | Step=2601/14375\nEpoch  2: Loss=88.5033 | Val MAE=84.70 | LR=2.00e-04 | Time=0:17:08 | Step=2651/14375\nEpoch  2: Loss=81.3896 | Val MAE=84.70 | LR=2.00e-04 | Time=0:17:28 | Step=2701/14375\nEpoch  2: Loss=85.8810 | Val MAE=84.70 | LR=2.00e-04 | Time=0:17:47 | Step=2751/14375\nEpoch  2: Loss=84.9497 | Val MAE=84.70 | LR=2.00e-04 | Time=0:18:06 | Step=2801/14375\nEpoch  2: Loss=83.1274 | Val MAE=84.70 | LR=2.00e-04 | Time=0:18:25 | Step=2851/14375\nEpoch  2: Loss=82.7661 | Val MAE=84.70 | LR=2.00e-04 | Time=0:18:45 | Step=2901/14375\nEpoch  2: Loss=86.4412 | Val MAE=84.70 | LR=2.00e-04 | Time=0:19:04 | Step=2951/14375\nEpoch  2: Loss=85.8306 | Val MAE=84.70 | LR=2.00e-04 | Time=0:19:23 | Step=3001/14375\nEpoch  2: Loss=86.9905 | Val MAE=84.70 | LR=2.00e-04 | Time=0:19:43 | Step=3051/14375\nEpoch  2: Loss=91.2747 | Val MAE=84.70 | LR=2.00e-04 | Time=0:20:02 | Step=3101/14375\nEpoch  2: Loss=91.9295 | Val MAE=84.70 | LR=2.00e-04 | Time=0:20:22 | Step=3151/14375\nEpoch  2: Loss=84.1205 | Val MAE=84.70 | LR=2.00e-04 | Time=0:20:41 | Step=3201/14375\nEpoch  2: Loss=79.6185 | Val MAE=84.70 | LR=2.00e-04 | Time=0:21:01 | Step=3251/14375\nEpoch  2: Loss=81.0429 | Val MAE=84.70 | LR=2.00e-04 | Time=0:21:21 | Step=3301/14375\nEpoch  2: Loss=81.3912 | Val MAE=84.70 | LR=2.00e-04 | Time=0:21:40 | Step=3351/14375\nEpoch  2: Loss=84.2746 | Val MAE=84.70 | LR=2.00e-04 | Time=0:21:59 | Step=3401/14375\nEpoch  2: Loss=86.1532 | Val MAE=84.70 | LR=2.00e-04 | Time=0:22:18 | Step=3451/14375\nEpoch  2: Loss=95.8820 | Val MAE=84.70 | LR=2.00e-04 | Time=0:22:38 | Step=3501/14375\nEpoch  2: Loss=87.1439 | Val MAE=84.70 | LR=2.00e-04 | Time=0:22:57 | Step=3551/14375\nEpoch  2: Loss=79.3018 | Val MAE=84.70 | LR=2.00e-04 | Time=0:23:17 | Step=3601/14375\nEpoch  2: Loss=75.6786 | Val MAE=84.70 | LR=2.00e-04 | Time=0:23:36 | Step=3651/14375\nEpoch  2: Loss=82.2226 | Val MAE=84.70 | LR=2.00e-04 | Time=0:23:55 | Step=3701/14375\nEpoch  2: Loss=80.3090 | Val MAE=84.70 | LR=2.00e-04 | Time=0:24:15 | Step=3751/14375\nEpoch  2: Loss=76.8325 | Val MAE=84.70 | LR=2.00e-04 | Time=0:24:34 | Step=3801/14375\nEpoch  2: Loss=79.9463 | Val MAE=84.70 | LR=2.00e-04 | Time=0:24:54 | Step=3851/14375\nEpoch  2: Loss=86.7088 | Val MAE=84.70 | LR=2.00e-04 | Time=0:25:13 | Step=3901/14375\nEpoch  2: Loss=87.8074 | Val MAE=84.70 | LR=2.00e-04 | Time=0:25:32 | Step=3951/14375\nEpoch  2: Loss=88.7689 | Val MAE=84.70 | LR=2.00e-04 | Time=0:25:52 | Step=4001/14375\nEpoch  2: Loss=83.0882 | Val MAE=84.70 | LR=2.00e-04 | Time=0:26:11 | Step=4051/14375\nEpoch  2: Loss=87.1345 | Val MAE=84.70 | LR=2.00e-04 | Time=0:26:31 | Step=4101/14375\nEpoch  2: Loss=84.6815 | Val MAE=84.70 | LR=2.00e-04 | Time=0:26:50 | Step=4151/14375\nEpoch  2: Loss=83.5292 | Val MAE=84.70 | LR=2.00e-04 | Time=0:27:10 | Step=4201/14375\nEpoch  2: Loss=82.9233 | Val MAE=84.70 | LR=2.00e-04 | Time=0:27:29 | Step=4251/14375\nEpoch  2: Loss=83.1973 | Val MAE=84.70 | LR=2.00e-04 | Time=0:27:49 | Step=4301/14375\nEpoch  2: Loss=79.4551 | Val MAE=84.70 | LR=2.00e-04 | Time=0:28:08 | Step=4351/14375\nEpoch  2: Loss=79.7603 | Val MAE=84.70 | LR=2.00e-04 | Time=0:28:28 | Step=4401/14375\nEpoch  2: Loss=79.6691 | Val MAE=84.70 | LR=2.00e-04 | Time=0:28:47 | Step=4451/14375\nEpoch  2: Loss=79.0016 | Val MAE=84.70 | LR=2.00e-04 | Time=0:29:06 | Step=4501/14375\nEpoch  2: Loss=86.8770 | Val MAE=84.70 | LR=2.00e-04 | Time=0:29:26 | Step=4551/14375\nEpoch  2: Loss=88.7956 | Val MAE=84.70 | LR=2.00e-04 | Time=0:29:45 | Step=4601/14375\nEpoch  2: Loss=82.9645 | Val MAE=84.70 | LR=2.00e-04 | Time=0:30:04 | Step=4651/14375\nEpoch  2: Loss=87.7393 | Val MAE=84.70 | LR=2.00e-04 | Time=0:30:24 | Step=4701/14375\nEpoch  2: Loss=85.5429 | Val MAE=84.70 | LR=2.00e-04 | Time=0:30:43 | Step=4751/14375\nEpoch  2: Loss=85.1583 | Val MAE=84.70 | LR=2.00e-04 | Time=0:31:02 | Step=4801/14375\nEpoch  2: Loss=82.4894 | Val MAE=84.70 | LR=2.00e-04 | Time=0:31:22 | Step=4851/14375\nEpoch  2: Loss=82.0813 | Val MAE=84.70 | LR=2.00e-04 | Time=0:31:41 | Step=4901/14375\nEpoch  2: Loss=76.3552 | Val MAE=84.70 | LR=2.00e-04 | Time=0:32:01 | Step=4951/14375\nEpoch  2: Loss=81.7368 | Val MAE=84.70 | LR=2.00e-04 | Time=0:32:20 | Step=5001/14375\nEpoch  2: Loss=84.7840 | Val MAE=84.70 | LR=2.00e-04 | Time=0:32:40 | Step=5051/14375\nEpoch  2: Loss=80.7123 | Val MAE=84.70 | LR=2.00e-04 | Time=0:32:59 | Step=5101/14375\nEpoch  2: Loss=79.6525 | Val MAE=84.70 | LR=2.00e-04 | Time=0:33:18 | Step=5151/14375\nEpoch  2: Loss=82.7730 | Val MAE=84.70 | LR=2.00e-04 | Time=0:33:38 | Step=5201/14375\nEpoch  2: Loss=79.6149 | Val MAE=84.70 | LR=2.00e-04 | Time=0:33:57 | Step=5251/14375\nEpoch  2: Loss=76.1755 | Val MAE=84.70 | LR=2.00e-04 | Time=0:34:16 | Step=5301/14375\nEpoch  2: Loss=89.1935 | Val MAE=84.70 | LR=2.00e-04 | Time=0:34:35 | Step=5351/14375\nEpoch  2: Loss=81.4162 | Val MAE=84.70 | LR=2.00e-04 | Time=0:34:55 | Step=5401/14375\nEpoch  2: Loss=84.7928 | Val MAE=84.70 | LR=2.00e-04 | Time=0:35:14 | Step=5451/14375\nEpoch  2: Loss=83.5409 | Val MAE=84.70 | LR=2.00e-04 | Time=0:35:33 | Step=5501/14375\nEpoch  2: Loss=75.8354 | Val MAE=84.70 | LR=2.00e-04 | Time=0:35:53 | Step=5551/14375\nEpoch  2: Loss=77.2417 | Val MAE=84.70 | LR=2.00e-04 | Time=0:36:12 | Step=5601/14375\nEpoch  2: Loss=76.8263 | Val MAE=84.70 | LR=2.00e-04 | Time=0:36:32 | Step=5651/14375\nEpoch  2: Loss=78.4064 | Val MAE=84.70 | LR=2.00e-04 | Time=0:36:51 | Step=5701/14375\nEpoch  2: Loss=76.9009 | Val MAE=84.70 | LR=2.00e-04 | Time=0:37:10 | Step=5751/14375\nEpoch  2: Loss=82.3354 | Val MAE=84.70 | LR=2.00e-04 | Time=0:37:30 | Step=5801/14375\nEpoch  2: Loss=78.6428 | Val MAE=84.70 | LR=2.00e-04 | Time=0:37:49 | Step=5851/14375\nEpoch  2: Loss=88.3777 | Val MAE=84.70 | LR=2.00e-04 | Time=0:38:08 | Step=5901/14375\nEpoch  2: Loss=78.4848 | Val MAE=84.70 | LR=2.00e-04 | Time=0:38:28 | Step=5951/14375\nEpoch  2: Loss=79.3408 | Val MAE=84.70 | LR=2.00e-04 | Time=0:38:47 | Step=6001/14375\nEpoch  2: Loss=85.6116 | Val MAE=84.70 | LR=2.00e-04 | Time=0:39:06 | Step=6051/14375\nEpoch  2: Loss=82.8490 | Val MAE=84.70 | LR=2.00e-04 | Time=0:39:26 | Step=6101/14375\nEpoch  2: Loss=85.6490 | Val MAE=84.70 | LR=2.00e-04 | Time=0:39:45 | Step=6151/14375\nEpoch  2: Loss=81.8893 | Val MAE=84.70 | LR=2.00e-04 | Time=0:40:04 | Step=6201/14375\nEpoch  2: Loss=79.7076 | Val MAE=84.70 | LR=2.00e-04 | Time=0:40:24 | Step=6251/14375\nEpoch  2: Loss=79.0414 | Val MAE=84.70 | LR=2.00e-04 | Time=0:40:43 | Step=6301/14375\nEpoch  2: Loss=83.1200 | Val MAE=84.70 | LR=2.00e-04 | Time=0:41:03 | Step=6351/14375\nEpoch  2: Loss=79.4582 | Val MAE=84.70 | LR=2.00e-04 | Time=0:41:22 | Step=6401/14375\nEpoch  2: Loss=77.1258 | Val MAE=84.70 | LR=2.00e-04 | Time=0:41:42 | Step=6451/14375\nEpoch  2: Loss=81.3429 | Val MAE=84.70 | LR=2.00e-04 | Time=0:42:01 | Step=6501/14375\nEpoch  2: Loss=79.5091 | Val MAE=84.70 | LR=2.00e-04 | Time=0:42:20 | Step=6551/14375\nEpoch  2: Loss=79.4205 | Val MAE=84.70 | LR=2.00e-04 | Time=0:42:40 | Step=6601/14375\nEpoch  2: Loss=78.2389 | Val MAE=84.70 | LR=2.00e-04 | Time=0:42:59 | Step=6651/14375\nEpoch  2: Loss=80.2997 | Val MAE=84.70 | LR=2.00e-04 | Time=0:43:18 | Step=6701/14375\nEpoch  2: Loss=78.4944 | Val MAE=84.70 | LR=2.00e-04 | Time=0:43:38 | Step=6751/14375\nEpoch  2: Loss=82.7932 | Val MAE=84.70 | LR=2.00e-04 | Time=0:43:57 | Step=6801/14375\nEpoch  2: Loss=77.3696 | Val MAE=84.70 | LR=2.00e-04 | Time=0:44:16 | Step=6851/14375\nEpoch  2: Loss=87.5701 | Val MAE=84.70 | LR=2.00e-04 | Time=0:44:36 | Step=6901/14375\nEpoch  2: Loss=78.6112 | Val MAE=84.70 | LR=2.00e-04 | Time=0:44:55 | Step=6951/14375\nEpoch  2: Loss=87.7215 | Val MAE=84.70 | LR=2.00e-04 | Time=0:45:14 | Step=7001/14375\nEpoch  2: Loss=81.8331 | Val MAE=84.70 | LR=2.00e-04 | Time=0:45:34 | Step=7051/14375\nEpoch  2: Loss=81.7776 | Val MAE=84.70 | LR=2.00e-04 | Time=0:45:53 | Step=7101/14375\nEpoch  2: Loss=75.6608 | Val MAE=84.70 | LR=2.00e-04 | Time=0:46:12 | Step=7151/14375\nEpoch  2: Loss=73.0457 | Val MAE=84.70 | LR=2.00e-04 | Time=0:46:31 | Step=7201/14375\nEpoch  2: Loss=78.1844 | Val MAE=84.70 | LR=2.00e-04 | Time=0:46:51 | Step=7251/14375\nEpoch  2: Loss=83.7151 | Val MAE=84.70 | LR=2.00e-04 | Time=0:47:10 | Step=7301/14375\nEpoch  2: Loss=76.9830 | Val MAE=84.70 | LR=2.00e-04 | Time=0:47:29 | Step=7351/14375\nEpoch  2: Loss=80.1827 | Val MAE=84.70 | LR=2.00e-04 | Time=0:47:49 | Step=7401/14375\nEpoch  2: Loss=82.7670 | Val MAE=84.70 | LR=2.00e-04 | Time=0:48:08 | Step=7451/14375\nEpoch  2: Loss=77.0525 | Val MAE=84.70 | LR=2.00e-04 | Time=0:48:27 | Step=7501/14375\nEpoch  2: Loss=81.3958 | Val MAE=84.70 | LR=2.00e-04 | Time=0:48:47 | Step=7551/14375\nEpoch  2: Loss=77.9509 | Val MAE=84.70 | LR=2.00e-04 | Time=0:49:06 | Step=7601/14375\nEpoch  2: Loss=77.1177 | Val MAE=84.70 | LR=2.00e-04 | Time=0:49:26 | Step=7651/14375\nEpoch  2: Loss=78.1723 | Val MAE=84.70 | LR=2.00e-04 | Time=0:49:45 | Step=7701/14375\nEpoch  2: Loss=75.3005 | Val MAE=84.70 | LR=2.00e-04 | Time=0:50:04 | Step=7751/14375\nEpoch  2: Loss=78.2481 | Val MAE=84.70 | LR=2.00e-04 | Time=0:50:24 | Step=7801/14375\nEpoch  2: Loss=75.6362 | Val MAE=84.70 | LR=2.00e-04 | Time=0:50:43 | Step=7851/14375\nEpoch  2: Loss=80.1129 | Val MAE=84.70 | LR=2.00e-04 | Time=0:51:02 | Step=7901/14375\nEpoch  2: Loss=76.5886 | Val MAE=84.70 | LR=2.00e-04 | Time=0:51:22 | Step=7951/14375\nEpoch  2: Loss=77.8983 | Val MAE=84.70 | LR=2.00e-04 | Time=0:51:41 | Step=8001/14375\nEpoch  2: Loss=83.8093 | Val MAE=84.70 | LR=2.00e-04 | Time=0:52:00 | Step=8051/14375\nEpoch  2: Loss=78.7538 | Val MAE=84.70 | LR=2.00e-04 | Time=0:52:20 | Step=8101/14375\nEpoch  2: Loss=81.1695 | Val MAE=84.70 | LR=2.00e-04 | Time=0:52:40 | Step=8151/14375\nEpoch  2: Loss=77.5529 | Val MAE=84.70 | LR=2.00e-04 | Time=0:52:59 | Step=8201/14375\nEpoch  2: Loss=77.6259 | Val MAE=84.70 | LR=2.00e-04 | Time=0:53:18 | Step=8251/14375\nEpoch  2: Loss=78.2094 | Val MAE=84.70 | LR=2.00e-04 | Time=0:53:38 | Step=8301/14375\nEpoch  2: Loss=77.0426 | Val MAE=84.70 | LR=2.00e-04 | Time=0:53:57 | Step=8351/14375\nEpoch  2: Loss=78.3515 | Val MAE=84.70 | LR=2.00e-04 | Time=0:54:16 | Step=8401/14375\nEpoch  2: Loss=83.0496 | Val MAE=84.70 | LR=2.00e-04 | Time=0:54:36 | Step=8451/14375\nEpoch  2: Loss=79.2410 | Val MAE=84.70 | LR=2.00e-04 | Time=0:54:55 | Step=8501/14375\nEpoch  2: Loss=83.7741 | Val MAE=84.70 | LR=2.00e-04 | Time=0:55:14 | Step=8551/14375\nEpoch  2: Loss=92.8190 | Val MAE=84.70 | LR=2.00e-04 | Time=0:55:33 | Step=8601/14375\nEpoch  2: Loss=82.5380 | Val MAE=84.70 | LR=2.00e-04 | Time=0:55:53 | Step=8651/14375\nEpoch  2: Loss=91.6628 | Val MAE=84.70 | LR=2.00e-04 | Time=0:56:12 | Step=8701/14375\nEpoch  2: Loss=85.1538 | Val MAE=84.70 | LR=2.00e-04 | Time=0:56:31 | Step=8751/14375\nEpoch  2: Loss=81.7293 | Val MAE=84.70 | LR=2.00e-04 | Time=0:56:51 | Step=8801/14375\nEpoch  2: Loss=76.5740 | Val MAE=84.70 | LR=2.00e-04 | Time=0:57:10 | Step=8851/14375\nEpoch  2: Loss=81.6267 | Val MAE=84.70 | LR=2.00e-04 | Time=0:57:29 | Step=8901/14375\nEpoch  2: Loss=75.9724 | Val MAE=84.70 | LR=2.00e-04 | Time=0:57:49 | Step=8951/14375\nEpoch  2: Loss=73.2112 | Val MAE=84.70 | LR=2.00e-04 | Time=0:58:08 | Step=9001/14375\nEpoch  2: Loss=79.5795 | Val MAE=84.70 | LR=2.00e-04 | Time=0:58:27 | Step=9051/14375\nEpoch  2: Loss=75.3191 | Val MAE=84.70 | LR=2.00e-04 | Time=0:58:46 | Step=9101/14375\nEpoch  2: Loss=80.3303 | Val MAE=84.70 | LR=2.00e-04 | Time=0:59:06 | Step=9151/14375\nEpoch  2: Loss=74.4950 | Val MAE=84.70 | LR=2.00e-04 | Time=0:59:25 | Step=9201/14375\nEpoch  2: Loss=74.8272 | Val MAE=84.70 | LR=2.00e-04 | Time=0:59:44 | Step=9251/14375\nEpoch  2: Loss=81.0594 | Val MAE=84.70 | LR=2.00e-04 | Time=1:00:04 | Step=9301/14375\nEpoch  2: Loss=84.7879 | Val MAE=84.70 | LR=2.00e-04 | Time=1:00:23 | Step=9351/14375\nEpoch  2: Loss=85.9810 | Val MAE=84.70 | LR=2.00e-04 | Time=1:00:42 | Step=9401/14375\nEpoch  2: Loss=80.5595 | Val MAE=84.70 | LR=2.00e-04 | Time=1:01:02 | Step=9451/14375\nEpoch  2: Loss=87.9247 | Val MAE=84.70 | LR=2.00e-04 | Time=1:01:22 | Step=9501/14375\nEpoch  2: Loss=78.5976 | Val MAE=84.70 | LR=2.00e-04 | Time=1:01:41 | Step=9551/14375\nEpoch  2: Loss=80.1268 | Val MAE=84.70 | LR=2.00e-04 | Time=1:02:01 | Step=9601/14375\nEpoch  2: Loss=82.0649 | Val MAE=84.70 | LR=2.00e-04 | Time=1:02:20 | Step=9651/14375\nEpoch  2: Loss=76.2184 | Val MAE=84.70 | LR=2.00e-04 | Time=1:02:39 | Step=9701/14375\nEpoch  2: Loss=71.2784 | Val MAE=84.70 | LR=2.00e-04 | Time=1:02:59 | Step=9751/14375\nEpoch  2: Loss=71.1275 | Val MAE=84.70 | LR=2.00e-04 | Time=1:03:18 | Step=9801/14375\nEpoch  2: Loss=82.3649 | Val MAE=84.70 | LR=2.00e-04 | Time=1:03:38 | Step=9851/14375\nEpoch  2: Loss=74.1308 | Val MAE=84.70 | LR=2.00e-04 | Time=1:03:57 | Step=9901/14375\nEpoch  2: Loss=95.3474 | Val MAE=84.70 | LR=2.00e-04 | Time=1:04:16 | Step=9951/14375\nEpoch  2: Loss=80.3302 | Val MAE=84.70 | LR=2.00e-04 | Time=1:04:35 | Step=10001/14375\nEpoch  2: Loss=79.5893 | Val MAE=84.70 | LR=2.00e-04 | Time=1:04:55 | Step=10051/14375\nEpoch  2: Loss=76.8975 | Val MAE=84.70 | LR=2.00e-04 | Time=1:05:14 | Step=10101/14375\nEpoch  2: Loss=75.2760 | Val MAE=84.70 | LR=2.00e-04 | Time=1:05:34 | Step=10151/14375\nEpoch  2: Loss=75.7616 | Val MAE=84.70 | LR=2.00e-04 | Time=1:05:53 | Step=10201/14375\nEpoch  2: Loss=87.1228 | Val MAE=84.70 | LR=2.00e-04 | Time=1:06:12 | Step=10251/14375\nEpoch  2: Loss=81.6413 | Val MAE=84.70 | LR=2.00e-04 | Time=1:06:32 | Step=10301/14375\nEpoch  2: Loss=78.5673 | Val MAE=84.70 | LR=2.00e-04 | Time=1:06:51 | Step=10351/14375\nEpoch  2: Loss=75.4837 | Val MAE=84.70 | LR=2.00e-04 | Time=1:07:10 | Step=10401/14375\nEpoch  2: Loss=76.5069 | Val MAE=84.70 | LR=2.00e-04 | Time=1:07:30 | Step=10451/14375\nEpoch  2: Loss=85.9816 | Val MAE=84.70 | LR=2.00e-04 | Time=1:07:49 | Step=10501/14375\nEpoch  2: Loss=82.5660 | Val MAE=84.70 | LR=2.00e-04 | Time=1:08:08 | Step=10551/14375\nEpoch  2: Loss=80.0671 | Val MAE=84.70 | LR=2.00e-04 | Time=1:08:28 | Step=10601/14375\nEpoch  2: Loss=74.8609 | Val MAE=84.70 | LR=2.00e-04 | Time=1:08:47 | Step=10651/14375\nEpoch  2: Loss=73.7687 | Val MAE=84.70 | LR=2.00e-04 | Time=1:09:07 | Step=10701/14375\nEpoch  2: Loss=76.6610 | Val MAE=84.70 | LR=2.00e-04 | Time=1:09:26 | Step=10751/14375\nEpoch  2: Loss=86.4818 | Val MAE=84.70 | LR=2.00e-04 | Time=1:09:45 | Step=10801/14375\nEpoch  2: Loss=81.8456 | Val MAE=84.70 | LR=2.00e-04 | Time=1:10:05 | Step=10851/14375\nEpoch  2: Loss=87.5273 | Val MAE=84.70 | LR=2.00e-04 | Time=1:10:24 | Step=10901/14375\nEpoch  2: Loss=76.6581 | Val MAE=84.70 | LR=2.00e-04 | Time=1:10:43 | Step=10951/14375\nEpoch  2: Loss=81.4140 | Val MAE=84.70 | LR=2.00e-04 | Time=1:11:03 | Step=11001/14375\nEpoch  2: Loss=73.6953 | Val MAE=84.70 | LR=2.00e-04 | Time=1:11:22 | Step=11051/14375\nEpoch  2: Loss=76.3044 | Val MAE=84.70 | LR=2.00e-04 | Time=1:11:42 | Step=11101/14375\nEpoch  2: Loss=73.7214 | Val MAE=84.70 | LR=2.00e-04 | Time=1:12:01 | Step=11151/14375\nEpoch  2: Loss=71.8726 | Val MAE=84.70 | LR=2.00e-04 | Time=1:12:20 | Step=11201/14375\nEpoch  2: Loss=75.6993 | Val MAE=84.70 | LR=2.00e-04 | Time=1:12:40 | Step=11251/14375\nEpoch  2: Loss=73.7635 | Val MAE=84.70 | LR=2.00e-04 | Time=1:13:00 | Step=11301/14375\nEpoch  2: Loss=74.4422 | Val MAE=84.70 | LR=2.00e-04 | Time=1:13:19 | Step=11351/14375\nEpoch  2: Loss=69.3044 | Val MAE=84.70 | LR=2.00e-04 | Time=1:13:39 | Step=11401/14375\nEpoch  2: Loss=73.8641 | Val MAE=84.70 | LR=2.00e-04 | Time=1:13:58 | Step=11451/14375\nEpoch  2: Loss=73.5828 | Val MAE=84.70 | LR=2.00e-04 | Time=1:14:17 | Step=11501/14375\nEpoch  2: Loss=70.7211 | Val MAE=84.70 | LR=2.00e-04 | Time=1:14:37 | Step=11551/14375\nEpoch  2: Loss=75.9716 | Val MAE=84.70 | LR=2.00e-04 | Time=1:14:56 | Step=11601/14375\nEpoch  2: Loss=87.7689 | Val MAE=84.70 | LR=2.00e-04 | Time=1:15:15 | Step=11651/14375\nEpoch  2: Loss=83.9669 | Val MAE=84.70 | LR=2.00e-04 | Time=1:15:35 | Step=11701/14375\nEpoch  2: Loss=76.5824 | Val MAE=84.70 | LR=2.00e-04 | Time=1:15:54 | Step=11751/14375\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import glob\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom _cfg import cfg\nfrom _model import Net, EnsembleModel\n\nprint(\"=\"*60)\nprint(\"üì• LOADING TRAINED MODELS\")\nprint(\"=\"*60)\n\nif RUN_VALID or RUN_TEST:\n    models = []\n    \n    # Try to load our newly trained model first\n    trained_models = glob.glob(\"enhanced_model_best_*.pt\")\n    if trained_models:\n        print(\"üéØ Loading our enhanced trained model:\")\n        for f in sorted(trained_models):\n            print(f\"   Loading: {f}\")\n            m = Net(\n                backbone=\"convnext_small.fb_in22k_ft_in1k\",\n                pretrained=False,\n            )\n            state_dict = torch.load(f, map_location=cfg.device, weights_only=True)\n            m.load_state_dict(state_dict)\n            models.append(m)\n    \n    # Also load pretrained baseline models for comparison if available\n    baseline_models = glob.glob(\"/kaggle/input/simple-further-finetuned-bartley-open-models/*.pth\")\n    if baseline_models and len(models) == 0:\n        print(\"üìÅ Loading baseline pretrained models:\")\n        for f in sorted(baseline_models):\n            print(f\"   Loading: {f}\")\n            m = Net(\n                backbone=\"convnext_small.fb_in22k_ft_in1k\",\n                pretrained=False,\n            )\n            state_dict = torch.load(f, map_location=cfg.device, weights_only=True)\n            state_dict = {k.removeprefix(\"_orig_mod.\"):v for k,v in state_dict.items()}\n            m.load_state_dict(state_dict)\n            models.append(m)\n    \n    if not models:\n        print(\"‚ùå No models found! Train a model first.\")\n        RUN_VALID = False\n        RUN_TEST = False\n    else:\n        # Create ensemble\n        model = EnsembleModel(models)\n        model = model.to(cfg.device)\n        model = model.eval()\n        print(f\"‚úÖ Loaded {len(models)} model(s) for ensemble\")\n        print(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if RUN_VALID:\n    print(\"üìä VALIDATION ANALYSIS\")\n    print(\"=\"*60)\n    \n    from tqdm import tqdm\n    import numpy as np\n    from _dataset import CustomDataset\n\n    # Dataset / Dataloader\n    valid_ds = CustomDataset(cfg=cfg, mode=\"valid\")\n    sampler = torch.utils.data.SequentialSampler(valid_ds)\n    valid_dl = torch.utils.data.DataLoader(\n        valid_ds, \n        sampler=sampler,\n        batch_size=cfg.batch_size_val, \n        num_workers=4,\n    )\n\n    # Validation loop\n    criterion = nn.L1Loss()\n    val_logits = []\n    val_targets = []\n    \n    print(\"üîÑ Running validation...\")\n    with torch.no_grad():\n        for x, y in tqdm(valid_dl, desc=\"Validating\"):\n            x = x.to(cfg.device)\n            y = y.to(cfg.device)\n    \n            with torch.autocast(cfg.device.type):\n                out = model(x)\n    \n            val_logits.append(out.cpu())\n            val_targets.append(y.cpu())\n    \n        val_logits = torch.cat(val_logits, dim=0)\n        val_targets = torch.cat(val_targets, dim=0)\n    \n        total_loss = criterion(val_logits, val_targets).item()\n    \n    # Dataset-wise scores\n    ds_idxs = np.array([valid_ds.records])\n    ds_idxs = np.repeat(ds_idxs, repeats=500)\n    \n    print(\"=\"*60)\n    print(\"üìà VALIDATION RESULTS\")\n    print(\"=\"*60)\n    print(\"Dataset-wise MAE Scores:\")\n    print(\"-\" * 30)\n    \n    dataset_scores = {}\n    with torch.no_grad():    \n        for idx in sorted(np.unique(ds_idxs)):\n            mask = ds_idxs == idx\n            logits_ds = val_logits[mask]\n            targets_ds = val_targets[mask]\n    \n            loss = criterion(val_logits[mask], val_targets[mask]).item()\n            dataset_scores[idx] = loss\n            print(f\"{idx:15} {loss:6.2f}\")\n    \n    print(\"-\" * 30)\n    print(f\"üéØ OVERALL VAL MAE: {total_loss:.3f}\")\n    print(\"=\"*60)\n    \n    # Comparison with baseline (if we know baseline scores)\n    baseline_scores = {\n        'CurveFault_A': 4.57,\n        'CurveFault_B': 88.19,\n        'CurveVel_A': 12.23,\n        'CurveVel_B': 46.92,\n        'FlatFault_A': 2.99,\n        'FlatFault_B': 33.53,\n        'FlatVel_A': 1.62,\n        'FlatVel_B': 8.71,\n        'Style_A': 35.01,\n        'Style_B': 55.71\n    }\n    baseline_overall = 28.95\n    \n    print(\"üìä COMPARISON WITH BASELINE\")\n    print(\"=\"*60)\n    print(\"Dataset               Enhanced    Baseline    Improvement\")\n    print(\"-\" * 55)\n    \n    improvements = []\n    for dataset in sorted(dataset_scores.keys()):\n        if dataset in baseline_scores:\n            enhanced = dataset_scores[dataset]\n            baseline = baseline_scores[dataset]\n            improvement = baseline - enhanced\n            improvements.append(improvement)\n            status = \"‚úÖ\" if improvement > 0 else \"‚ùå\"\n            print(f\"{dataset:15} {enhanced:8.2f} {baseline:8.2f} {improvement:8.2f} {status}\")\n    \n    overall_improvement = baseline_overall - total_loss\n    print(\"-\" * 55)\n    print(f\"{'OVERALL':15} {total_loss:8.2f} {baseline_overall:8.2f} {overall_improvement:8.2f} {'‚úÖ' if overall_improvement > 0 else '‚ùå'}\")\n    \n    if overall_improvement > 0:\n        improvement_pct = (overall_improvement / baseline_overall) * 100\n        print(f\"üéâ IMPROVEMENT: {improvement_pct:.1f}% better than baseline!\")\n    else:\n        degradation_pct = abs(overall_improvement / baseline_overall) * 100\n        print(f\"‚ö†Ô∏è  DEGRADATION: {degradation_pct:.1f}% worse than baseline\")\n    \n    print(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if RUN_TEST:\n    print(\"üîÆ GENERATING TEST PREDICTIONS\")\n    print(\"=\"*60)\n    \n    import csv\n    import time\n    import pandas as pd\n    from _utils import format_time\n\n    class TestDataset(torch.utils.data.Dataset):\n        def __init__(self, test_files):\n            self.test_files = test_files\n\n        def __len__(self):\n            return len(self.test_files)\n\n        def __getitem__(self, i):\n            test_file = self.test_files[i]\n            test_stem = test_file.split(\"/\")[-1].split(\".\")[0]\n            return np.load(test_file), test_stem\n\n    ss = pd.read_csv(\"/kaggle/input/waveform-inversion/sample_submission.csv\")    \n    row_count = 0\n    t0 = time.time()\n    \n    test_files = sorted(glob.glob(\"/kaggle/input/open-wfi-test/test/*.npy\"))\n    x_cols = [f\"x_{i}\" for i in range(1, 70, 2)]\n    fieldnames = [\"oid_ypos\"] + x_cols\n    \n    test_ds = TestDataset(test_files)\n    test_dl = torch.utils.data.DataLoader(\n        test_ds, \n        sampler=torch.utils.data.SequentialSampler(test_ds),\n        batch_size=cfg.batch_size_val, \n        num_workers=4,\n    )\n    \n    print(f\"üìÅ Found {len(test_files)} test files\")\n    print(\"üîÑ Generating predictions...\")\n    \n    with open(\"enhanced_submission.csv\", \"wt\", newline=\"\") as csvfile:\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        with torch.inference_mode():\n            with torch.autocast(cfg.device.type):\n                for inputs, oids_test in tqdm(test_dl, desc=\"Predicting\"):\n                    inputs = inputs.to(cfg.device)\n            \n                    outputs = model(inputs)\n                            \n                    y_preds = outputs[:, 0].cpu().numpy()\n                    \n                    for y_pred, oid_test in zip(y_preds, oids_test):\n                        for y_pos in range(70):\n                            row = dict(zip(x_cols, [y_pred[y_pos, x_pos] for x_pos in range(1, 70, 2)]))\n                            row[\"oid_ypos\"] = f\"{oid_test}_y_{y_pos}\"\n            \n                            writer.writerow(row)\n                            row_count += 1\n\n                            if row_count % 100_000 == 0:\n                                csvfile.flush()\n    \n    t1 = format_time(time.time() - t0)\n    print(f\"‚è±Ô∏è  Inference Time: {t1}\")\n    print(f\"üìù Generated {row_count:,} predictions\")\n    print(\"üíæ Saved to: enhanced_submission.csv\")\n    print(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if RUN_TEST:\n    print(\"üé® PREDICTION VISUALIZATION\")\n    print(\"=\"*60)\n    \n    import matplotlib.pyplot as plt \n\n    # Plot sample predictions\n    fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n    axes = axes.flatten()\n\n    n = min(len(outputs), len(axes))\n    \n    for i in range(n):\n        img = outputs[i, 0].cpu().numpy()\n        idx = oids_test[i]\n    \n        axes[i].imshow(img, cmap='seismic', aspect='auto')\n        axes[i].set_title(f'{idx}', fontsize=10)\n        axes[i].axis('off')\n\n    for i in range(n, len(axes)):\n        axes[i].axis('off')\n    \n    plt.suptitle('Enhanced Model Predictions - Velocity Models', fontsize=16, y=0.98)\n    plt.tight_layout()\n    plt.show()\n\nprint(\"=\"*60)\nprint(\"üéØ ENHANCED TRAINING COMPLETE!\")\nprint(\"=\"*60)\nprint(\"Key Improvements:\")\nprint(\"‚Ä¢ ‚úÖ L1+SSIM Loss Function\")\nprint(\"‚Ä¢ ‚úÖ AdamW Optimizer with Weight Decay\")\nprint(\"‚Ä¢ ‚úÖ Cosine Annealing LR Schedule\")\nprint(\"‚Ä¢ ‚úÖ Enhanced EMA (decay=0.999)\")\nprint(\"‚Ä¢ ‚úÖ Better Gradient Clipping\")\nprint(\"‚Ä¢ ‚úÖ Detailed Progress Tracking\")\nif 'total_loss' in locals():\n    print(f\"‚Ä¢ üéØ Final Validation MAE: {total_loss:.3f}\")\n    if 'baseline_overall' in locals():\n        improvement = baseline_overall - total_loss\n        if improvement > 0:\n            print(f\"‚Ä¢ üöÄ Improvement over baseline: {improvement:.3f} ({(improvement/baseline_overall)*100:.1f}%)\")\nprint(\"=\"*60)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}